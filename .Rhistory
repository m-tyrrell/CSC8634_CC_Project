heat_vis('Render','outlier',outlier_qty=25,caption='on')
# Filter and aggregate joined app_check/taskxy to display only relevant variables/tasks
heat_vis = function(event, metric, outlier_var = 'duration', outlier_qty = 1, caption='on'){
cap_label = paste(event,'',toTitleCase(metric),' by Tile')
if(caption == 'off'){
cap_label = ""
}
if(metric == 'outlier'){
cap_label = paste(event,'',toTitleCase(outlier_var),' by Tile - Filtered (',outlier_var,' less than ',outlier_qty,')')
}
comp_tile = app_task %>%
# Filter by selected event type and remove non-level 12 observations (because there are basically none compared to level 12)
filter(eventName == event, level == 12) %>%
# Aggregate by taskId computing duration of task using difftime (for each taskId)
group_by(taskId, eventName, x, y) %>%
summarise(duration = as.numeric(difftime(last(timestamp), first(timestamp), unit = 'sec'))) %>%
# Order df by row vectors to prepare for reordering tile durations by tile coordinates
arrange(x,y) %>%
# Join gpu_task dataset (computed means of resource usage by taskId and event)
left_join(gpu_task)
# Specify conditional filter for heatmap (can't use dplyr pipe because of fun argument input issues (lazyeval possible solution))
comp_tile$outlier = ifelse(comp_tile[[outlier_var]] < outlier_qty,1,1000)
# Split duration vector into 256 row vectors
x = split(comp_tile[[metric]], ceiling(seq_along(comp_tile[[metric]])/256))
# Call mapping function
map_matrix = build_map(x,256)
# Create heatmap of tile render durations
heatmap(map_matrix, Rowv=NA, Colv=NA, labRow=NA, labCol = NA, xlab = cap_label)
}
cache("heat_vis")
heat_vis('Render','outlier',outlier_qty=25,caption='on')
# Filter and aggregate joined app_check/taskxy to display only relevant variables/tasks
heat_vis = function(event, metric, outlier_var = 'duration', outlier_qty = 1, caption='on'){
cap_label = paste(event,'',toTitleCase(metric),' by Tile')
if(caption == 'off'){
cap_label = ""
}
h = '<'
if(metric == 'outlier'){
cap_label = paste(event,'',toTitleCase(outlier_var),' by Tile - Filtered (',outlier_var,h,outlier_qty,')')
}
comp_tile = app_task %>%
# Filter by selected event type and remove non-level 12 observations (because there are basically none compared to level 12)
filter(eventName == event, level == 12) %>%
# Aggregate by taskId computing duration of task using difftime (for each taskId)
group_by(taskId, eventName, x, y) %>%
summarise(duration = as.numeric(difftime(last(timestamp), first(timestamp), unit = 'sec'))) %>%
# Order df by row vectors to prepare for reordering tile durations by tile coordinates
arrange(x,y) %>%
# Join gpu_task dataset (computed means of resource usage by taskId and event)
left_join(gpu_task)
# Specify conditional filter for heatmap (can't use dplyr pipe because of fun argument input issues (lazyeval possible solution))
comp_tile$outlier = ifelse(comp_tile[[outlier_var]] < outlier_qty,1,1000)
# Split duration vector into 256 row vectors
x = split(comp_tile[[metric]], ceiling(seq_along(comp_tile[[metric]])/256))
# Call mapping function
map_matrix = build_map(x,256)
# Create heatmap of tile render durations
heatmap(map_matrix, Rowv=NA, Colv=NA, labRow=NA, labCol = NA, xlab = cap_label)
}
cache("heat_vis")
heat_vis('Render','outlier',outlier_qty=25,caption='on')
# Filter and aggregate joined app_check/taskxy to display only relevant variables/tasks
heat_vis = function(event, metric, outlier_var = 'duration', out_sign='<',outlier_qty = 1, caption='on'){
cap_label = paste(event,'',toTitleCase(metric),' by Tile')
if(caption == 'off'){
cap_label = ""
}
if(metric == 'outlier'){
cap_label = paste(event,'',toTitleCase(outlier_var),' by Tile - Filtered (',outlier_var,out_sign,outlier_qty,')')
}
comp_tile = app_task %>%
# Filter by selected event type and remove non-level 12 observations (because there are basically none compared to level 12)
filter(eventName == event, level == 12) %>%
# Aggregate by taskId computing duration of task using difftime (for each taskId)
group_by(taskId, eventName, x, y) %>%
summarise(duration = as.numeric(difftime(last(timestamp), first(timestamp), unit = 'sec'))) %>%
# Order df by row vectors to prepare for reordering tile durations by tile coordinates
arrange(x,y) %>%
# Join gpu_task dataset (computed means of resource usage by taskId and event)
left_join(gpu_task)
# Specify conditional filter for heatmap (can't use dplyr pipe because of fun argument input issues (lazyeval possible solution))
comp_tile$outlier = ifelse(comp_tile[[outlier_var]] < outlier_qty,1,1000)
# Split duration vector into 256 row vectors
x = split(comp_tile[[metric]], ceiling(seq_along(comp_tile[[metric]])/256))
# Call mapping function
map_matrix = build_map(x,256)
# Create heatmap of tile render durations
heatmap(map_matrix, Rowv=NA, Colv=NA, labRow=NA, labCol = NA, xlab = cap_label)
}
cache("heat_vis")
heat_vis('Render','outlier',outlier_qty=25,caption='on')
png('graphs/p28.png')
heat_vis('Render','outlier',outlier_qty=25,caption='on')
dev.off ();
i5 <-  rasterGrob(as.raster(readPNG("graphs/p28.png")), interpolate = FALSE)
cache('i5')
# Plot images
grid.arrange(i1, i3, i7, i9, i5, ncol = 3)
heat_vis('Render','outlier',outlier_qty=25,out_sign='<',caption='on')
heat_vis = function(event, metric, outlier_var = 'duration', out_sign='<',outlier_qty = 1, caption='on'){
cap_label = paste(event,'',toTitleCase(metric),' by Tile')
if(caption == 'off'){
cap_label = ""
}
if(metric == 'outlier'){
cap_label = paste(event,'',toTitleCase(outlier_var),' by Tile - Filtered (',outlier_var,out_sign,outlier_qty,')')
}
comp_tile = app_task %>%
# Filter by selected event type and remove non-level 12 observations (because there are basically none compared to level 12)
filter(eventName == event, level == 12) %>%
# Aggregate by taskId computing duration of task using difftime (for each taskId)
group_by(taskId, eventName, x, y) %>%
summarise(duration = as.numeric(difftime(last(timestamp), first(timestamp), unit = 'sec'))) %>%
# Order df by row vectors to prepare for reordering tile durations by tile coordinates
arrange(x,y) %>%
# Join gpu_task dataset (computed means of resource usage by taskId and event)
left_join(gpu_task)
# Specify conditional filter for heatmap (can't use dplyr pipe because of fun argument input issues (lazyeval possible solution))
comp_tile$outlier = ifelse(comp_tile[[outlier_var]] > outlier_qty,1,1000)
# Split duration vector into 256 row vectors
x = split(comp_tile[[metric]], ceiling(seq_along(comp_tile[[metric]])/256))
# Call mapping function
map_matrix = build_map(x,256)
# Create heatmap of tile render durations
heatmap(map_matrix, Rowv=NA, Colv=NA, labRow=NA, labCol = NA, xlab = cap_label)
}
cache("heat_vis")
heat_vis('Render','outlier',outlier_qty=60,out_sign='>',caption='on')
heat_vis('Render','outlier',outlier_qty=50,out_sign='>',caption='on')
png('graphs/p27.png')
heat_vis('Render','outlier',outlier_qty=50,out_sign='>',caption='on')
dev.off ();
png('graphs/p29.png')
heat_vis('Render','duration',caption='off')
dev.off ();
png('graphs/p30.png')
heat_vis('Render','watt',caption='on')
dev.off ();
png('graphs/p31.png')
heat_vis('Render','outlier',outlier_qty=50,out_sign='>',caption='on')
dev.off ();
png('graphs/p33.png')
heat_vis('Render','cpu',caption='on')
dev.off ();
png('graphs/p34.png')
heat_vis('Render','mem',caption='on')
dev.off ();
# Filter and aggregate joined app_check/taskxy to display only relevant variables/tasks
heat_vis = function(event, metric, outlier_var = 'duration', out_sign='<',outlier_qty = 1, caption='on'){
cap_label = paste(event,'',toTitleCase(metric),' by Tile')
if(caption == 'off'){
cap_label = ""
}
if(metric == 'outlier'){
cap_label = paste(event,'',toTitleCase(outlier_var),' by Tile - Filtered (',outlier_var,out_sign,outlier_qty,')')
}
comp_tile = app_task %>%
# Filter by selected event type and remove non-level 12 observations (because there are basically none compared to level 12)
filter(eventName == event, level == 12) %>%
# Aggregate by taskId computing duration of task using difftime (for each taskId)
group_by(taskId, eventName, x, y) %>%
summarise(duration = as.numeric(difftime(last(timestamp), first(timestamp), unit = 'sec'))) %>%
# Order df by row vectors to prepare for reordering tile durations by tile coordinates
arrange(x,y) %>%
# Join gpu_task dataset (computed means of resource usage by taskId and event)
left_join(gpu_task)
# Specify conditional filter for heatmap (can't use dplyr pipe because of fun argument input issues (lazyeval possible solution))
comp_tile$outlier = ifelse(comp_tile[[outlier_var]] < outlier_qty,1,1000)
# Split duration vector into 256 row vectors
x = split(comp_tile[[metric]], ceiling(seq_along(comp_tile[[metric]])/256))
# Call mapping function
map_matrix = build_map(x,256)
# Create heatmap of tile render durations
heatmap(map_matrix, Rowv=NA, Colv=NA, labRow=NA, labCol = NA, xlab = cap_label)
}
cache("heat_vis")
heat_vis('Render','outlier',outlier_qty=25,out_sign='<',caption='on')
png('graphs/p32.png')
heat_vis('Render','outlier',outlier_qty=25,out_sign='<',caption='on')
dev.off ();
i1 <-  rasterGrob(as.raster(readPNG("graphs/p29.png")), interpolate = FALSE)
cache('i1')
i2 <-  rasterGrob(as.raster(readPNG("graphs/p30.png")), interpolate = FALSE)
cache('i2')
i3 <-  rasterGrob(as.raster(readPNG("graphs/p31.png")), interpolate = FALSE)
cache('i3')
i4 <-  rasterGrob(as.raster(readPNG("graphs/p32.png")), interpolate = FALSE)
cache('i4')
i5 <-  rasterGrob(as.raster(readPNG("graphs/p33.png")), interpolate = FALSE)
cache('i5')
i6 <-  rasterGrob(as.raster(readPNG("graphs/p34.png")), interpolate = FALSE)
cache('i6')
# Plot images
grid.arrange(i4, i5, i6, i1, i2, i3, ncol = 3)
png('graphs/p29.png')
heat_vis('Render','duration',caption='on')
dev.off ();
i29 <-  rasterGrob(as.raster(readPNG("graphs/p29.png")), interpolate = FALSE)
cache('i29')
i30 <-  rasterGrob(as.raster(readPNG("graphs/p30.png")), interpolate = FALSE)
cache('i30')
i31 <-  rasterGrob(as.raster(readPNG("graphs/p31.png")), interpolate = FALSE)
cache('i31')
i32 <-  rasterGrob(as.raster(readPNG("graphs/p32.png")), interpolate = FALSE)
cache('i32')
i33 <-  rasterGrob(as.raster(readPNG("graphs/p33.png")), interpolate = FALSE)
cache('i33')
i34 <-  rasterGrob(as.raster(readPNG("graphs/p34.png")), interpolate = FALSE)
cache('i34')
# Plot images
grid.arrange(i29, i31, i32, i30, i33, i34, ncol = 3)
# Plot images
grid.arrange(i30, i33, i34, i29, i31, i32, ncol = 3)
View(gpu_task)
View(task_runtimes)
h = median(comp_gpu$avg_dur)
p24 = ggplot(comp_gpu) + geom_point(aes(gpuSerial, duration)) + labs(x = 'GPU S/N', y = 'Mean Render Time (s)') +
geom_hline(yintercept = h, color = 'red', linetype = 'dotdash') +
annotate("text", x=comp_gpu$gpuSerial[1]+500000000, y=h, label= paste("Median",round(h,2)), size=2.5, vjust=1.5)
cache('p24')
p24
ggplot(comp_gpu) + geom_point(aes(gpuSerial, avg_dur)) + labs(x = 'GPU S/N', y = 'Mean Render Time (s)') +
geom_hline(yintercept = h, color = 'red', linetype = 'dotdash') +
annotate("text", x=comp_gpu$gpuSerial[1]+500000000, y=h, label= paste("Median",round(h,2)), size=2.5, vjust=1.5)
h = median(comp_gpu$avg_dur)
p24 = ggplot(comp_gpu) + geom_point(aes(gpuSerial, avg_dur)) + labs(x = 'GPU S/N', y = 'Mean Render Time (s)') +
geom_hline(yintercept = h, color = 'red', linetype = 'dotdash') +
annotate("text", x=comp_gpu$gpuSerial[1]+500000000, y=h, label= paste("Median",round(h,2)), size=2.5, vjust=1.5)
h
comp_gpu = task_runtimes %>%
# Remove non-render tasks
filter(eventName == 'Render') %>%
# Join stripped down gpu dataset on hostname
left_join(gpu_serial, by = 'hostname') %>%
# Aggregate by mean render time
group_by(gpuSerial) %>%
summarise(avg_dur = mean(duration)) %>%
# Add index column for plotting
mutate(index = 1:1024)
cache('comp_gpu')
h = median(comp_gpu$avg_dur)
p24 = ggplot(comp_gpu) + geom_point(aes(gpuSerial, avg_dur)) + labs(x = 'GPU S/N', y = 'Mean Render Time (s)') +
geom_hline(yintercept = h, color = 'red', linetype = 'dotdash') +
annotate("text", x=comp_gpu$gpuSerial[1]+500000000, y=h, label= paste("Median",round(h,2)), size=2.5, vjust=1.5)
p24
test = task_runtimes %>%
# Join stripped down gpu dataset on hostname
left_join(gpu_task, by = 'taskId') %>%
)
# Filter and aggregate joined app_check/taskxy to display only relevant variables/tasks
test = task_runtimes %>%
# Join stripped down gpu dataset on hostname
left_join(gpu_task, by = 'taskId')
View(test)
# Filter and aggregate joined app_check/taskxy to display only relevant variables/tasks
test = task_runtimes %>%
# Join stripped down gpu dataset on hostname
left_join(gpu_task, by = by = c("taskId" = "taskId", "eventName" = "eventName"))
# Filter and aggregate joined app_check/taskxy to display only relevant variables/tasks
test = task_runtimes %>%
# Join stripped down gpu dataset on hostname
left_join(gpu_task, by = c("taskId" = "taskId", "eventName" = "eventName"))
View(test)
lenght(is.na(test$hostname.y))
length(is.na(test$hostname.y))
sum(is.na(test$hostname.y))
length(is.na(test$hostname.y)) - sum(is.na(test$hostname.y))
# Filter and aggregate joined app_check/taskxy to display only relevant variables/tasks
test = task_runtimes %>%
select(-hostname) %>%
# Join stripped down gpu dataset on hostname
left_join(gpu_task, by = c("taskId" = "taskId", "eventName" = "eventName"))
task_master = task_runtimes %>%
select(-hostname) %>%
# Join stripped down gpu dataset on hostname
left_join(gpu_task, by = c("taskId" = "taskId", "eventName" = "eventName"))
task_render = filter(task_master, eventName = 'Render')
task_render = filter(task_master, eventName == 'Render')
ggplot(task_render) + geom_point(aes()) + labs(x = 'temp', y = 'duration')
ggplot(task_render) + geom_point(aes(x = 'temp', y = 'duration')) + labs(x = 'temp', y = 'duration')
View(task_render)
ggplot(task_render) + geom_point(aes(x = temp, y = duration)) + labs(x = 'temp', y = 'duration')
ggplot(task_render) + geom_point(aes(x = temp, y = duration)) + labs(x = 'temp', y = 'duration') +
geom_point(size=0.25) + stat_smooth()
ggplot(task_render, aes(temp, duration)) + geom_line(color='#0066CC') + labs( x = 'timestamp (s)', y = 'Power (W)')
gpu_id = data.frame(gpuSerial = sort(unique(gpu$gpuSerial)), id =  1:1024)
# Create a copy of gpu dataset and insert gpu index vector
gpu_r = gpu
gpu_r = left_join(gpu_r, gpu_id)
# Filter gpu_r dataset by unique gpu index number for plotting and first 10 minutes
gpu_t = gpu_r %>%
filter(id == 500) %>%
arrange(timestamp) %>%
filter(timestamp < min(timestamp)+600)
ggplot(task_render, aes(temp, duration)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Power (W)', y = 'Temperature (C)')
ggplot(task_render, aes(watt, duration)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Power (W)', y = 'Temperature (C)')
ggplot(task_render, aes(mem, duration)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Power (W)', y = 'Temperature (C)')
ggplot(task_render, aes(cpu, duration)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Power (W)', y = 'Temperature (C)')
cache('task_render')
#Plot invidual correlations
p35 = ggplot(task_render, aes(watt, duration)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Power (W)', y = 'Temperature (C)')
p36 = ggplot(task_render, aes(temp, duration)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Power (W)', y = 'Temperature (C)')
p37 = ggplot(task_render, aes(cpu, duration)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Power (W)', y = 'Temperature (C)')
p38 = ggplot(task_render, aes(mem, duration)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Power (W)', y = 'Temperature (C)')
cache('p35')
cache('p36')
cache('p37')
cache('p38')
# Plot images
grid.arrange(p35, p36, p37, p38) ncol = 2)
# Plot images
grid.arrange(p35, p36, p37, p38, ncol = 2)
p35 = ggplot(task_render, aes(watt, duration)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Power (W)', y = 'Execution Time (s)')
p36 = ggplot(task_render, aes(temp, duration)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Temperature (C)', y = 'Execution Time (s)')
p37 = ggplot(task_render, aes(cpu, duration)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'GPU Usage (%)', y = 'Execution Time (s)')
p38 = ggplot(task_render, aes(mem, duration)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Memory Usage (%)', y = 'Execution Time (s)')
cache('p35')
cache('p36')
cache('p37')
cache('p38')
# Plot images
grid.arrange(p35, p36, p37, p38, ncol = 2)
p39 = grid.arrange(p35, p36, p37, p38, ncol = 2)
ggsave('graphs/p39.png', p39, width = 6.3, height = 5.8)
# Plot grid of 6
p11 = pgrid.arrange(p5, p6, p7, p8, p9, p10, ncol=3)
# Plot grid of 6
p11 = grid.arrange(p5, p6, p7, p8, p9, p10, ncol=3)
p5 = ggplot(q2_samp, aes(powerDrawWatt, gpuTempC)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Power (W)', y = 'Temperature (C)')
p6 = ggplot(q2_samp, aes(gpuUtilPerc, gpuTempC)) + geom_point(size=0.25) + stat_smooth()+ labs( x = 'GPU Usage (%)', y = 'Temperature (C)')
p7 = ggplot(q2_samp, aes(gpuUtilPerc, powerDrawWatt)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'GPU Usage (%)', y = 'Power (W)')
p8 = ggplot(q2_samp, aes(gpuUtilPerc, gpuMemUtilPerc)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'GPU Usage (%)', y = 'Memory Usage (%)')
p9 = ggplot(q2_samp, aes(powerDrawWatt, gpuMemUtilPerc)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Power (W)', y = 'Memory Usage (%)')
p10 = ggplot(q2_samp, aes(gpuTempC, gpuMemUtilPerc)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Temperature (C)', y = 'Memory Usage (%)')
# Plot grid of 6
# Plot grid of 6
p11 = grid.arrange(p5, p6, p7, p8, p9, p10, ncol=3)
# Q2 interplay between GPU Performance metrics
q2 = gpu %>%
select(4:7)
# Take sample of q2 index
x = sample(1:dim(q2)[1],1000)
# Slice q2 by sample
q2_samp = q2[x,]
p5 = ggplot(q2_samp, aes(powerDrawWatt, gpuTempC)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Power (W)', y = 'Temperature (C)')
p6 = ggplot(q2_samp, aes(gpuUtilPerc, gpuTempC)) + geom_point(size=0.25) + stat_smooth()+ labs( x = 'GPU Usage (%)', y = 'Temperature (C)')
p7 = ggplot(q2_samp, aes(gpuUtilPerc, powerDrawWatt)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'GPU Usage (%)', y = 'Power (W)')
p8 = ggplot(q2_samp, aes(gpuUtilPerc, gpuMemUtilPerc)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'GPU Usage (%)', y = 'Memory Usage (%)')
p9 = ggplot(q2_samp, aes(powerDrawWatt, gpuMemUtilPerc)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Power (W)', y = 'Memory Usage (%)')
p10 = ggplot(q2_samp, aes(gpuTempC, gpuMemUtilPerc)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Temperature (C)', y = 'Memory Usage (%)')
# Plot grid of 6
p11 = grid.arrange(p5, p6, p7, p8, p9, p10, ncol=3)
ggsave(filename="graphs/p11.png", p11)
p11 = grid.arrange(p5, p6, p7, p8, p9, p10, ncol=3)
ggsave('graphs/p11.png', p11, width = 6.3, height = 5.8)
ggsave('graphs/p11.png', p11, width = 6.3, height = 3.87)
p11 = grid.arrange(p5, p6, p7, p8, p9, p10, ncol=3)
ggsave('graphs/p11.png', p11, width = 6.3, height = 3.87)
cache('task_master')
grid.arrange(p35, p36, p37, p38, ncol = 2)
q2 = gpu %>%
select(4:7)
# Take sample of q2 index
x = sample(1:dim(q2)[1],10000)
# Slice q2 by sample
q2_samp = q2[x,]
p5 = ggplot(q2_samp, aes(powerDrawWatt, gpuTempC)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Power (W)', y = 'Temperature (C)')
p6 = ggplot(q2_samp, aes(gpuUtilPerc, gpuTempC)) + geom_point(size=0.25) + stat_smooth()+ labs( x = 'GPU Usage (%)', y = 'Temperature (C)')
p7 = ggplot(q2_samp, aes(gpuUtilPerc, powerDrawWatt)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'GPU Usage (%)', y = 'Power (W)')
p8 = ggplot(q2_samp, aes(gpuUtilPerc, gpuMemUtilPerc)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'GPU Usage (%)', y = 'Memory Usage (%)')
p9 = ggplot(q2_samp, aes(powerDrawWatt, gpuMemUtilPerc)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Power (W)', y = 'Memory Usage (%)')
p10 = ggplot(q2_samp, aes(gpuTempC, gpuMemUtilPerc)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Temperature (C)', y = 'Memory Usage (%)')
# Plot grid of 6
p11 = grid.arrange(p5, p6, p7, p8, p9, p10, ncol=3)
q2 = gpu %>%
select(4:7)
# Take sample of q2 index
x = sample(1:dim(q2)[1],100000)
# Slice q2 by sample
q2_samp = q2[x,]
p5 = ggplot(q2_samp, aes(powerDrawWatt, gpuTempC)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Power (W)', y = 'Temperature (C)')
p6 = ggplot(q2_samp, aes(gpuUtilPerc, gpuTempC)) + geom_point(size=0.25) + stat_smooth()+ labs( x = 'GPU Usage (%)', y = 'Temperature (C)')
p7 = ggplot(q2_samp, aes(gpuUtilPerc, powerDrawWatt)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'GPU Usage (%)', y = 'Power (W)')
p8 = ggplot(q2_samp, aes(gpuUtilPerc, gpuMemUtilPerc)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'GPU Usage (%)', y = 'Memory Usage (%)')
p9 = ggplot(q2_samp, aes(powerDrawWatt, gpuMemUtilPerc)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Power (W)', y = 'Memory Usage (%)')
p10 = ggplot(q2_samp, aes(gpuTempC, gpuMemUtilPerc)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Temperature (C)', y = 'Memory Usage (%)')
# Plot grid of 6
p11 = grid.arrange(p5, p6, p7, p8, p9, p10, ncol=3)
ggsave('graphs/p11.png', p11, width = 6.3, height = 3.87)
log_corr = gpu %>%
select(4:7)
# Take sample of q2 index
log_corr_samp = sample(1:dim(log_corr)[1],100000)
log_corr = gpu %>%
select(4:7)
# Take sample of q2 index
x = sample(1:dim(log_corr)[1],100000)
# Slice q2 by sample
log_corr_samp = log_corr_samp[x,]
log_corr = gpu %>%
select(4:7)
# Take sample of q2 index
x = sample(1:dim(log_corr)[1],100000)
# Slice q2 by sample
log_corr_samp = log_corr[x,]
p5 = ggplot(log_corr_samp, aes(powerDrawWatt, gpuTempC)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Power (W)', y = 'Temperature (C)')
p6 = ggplot(log_corr_samp, aes(gpuUtilPerc, gpuTempC)) + geom_point(size=0.25) + stat_smooth()+ labs( x = 'GPU Usage (%)', y = 'Temperature (C)')
p7 = ggplot(log_corr_samp, aes(gpuUtilPerc, powerDrawWatt)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'GPU Usage (%)', y = 'Power (W)')
p8 = ggplot(log_corr_samp, aes(gpuUtilPerc, gpuMemUtilPerc)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'GPU Usage (%)', y = 'Memory Usage (%)')
p9 = ggplot(log_corr_samp, aes(powerDrawWatt, gpuMemUtilPerc)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Power (W)', y = 'Memory Usage (%)')
p10 = ggplot(log_corr_samp, aes(gpuTempC, gpuMemUtilPerc)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Temperature (C)', y = 'Memory Usage (%)')
# Plot grid of 6
p11 = grid.arrange(p5, p6, p7, p8, p9, p10, ncol=3)
cache('log_corr_samp')
ggsave('graphs/p11.png', p11, width = 6.3, height = 3.87)
ggsave('graphs/p11.png', p11, width = 6.3, height = 3.87)
# Plot images
p39 = grid.arrange(p35, p36, p37, p38, ncol = 2)
View(task_render)
ggplot(comp_gpu) + geom_point(aes(gpuSerial, avg_dur)) + labs(x = 'GPU S/N', y = 'Mean Render Time (s)') +
geom_hline(yintercept = h, color = 'red', linetype = 'dotdash') +
annotate("text", x=comp_gpu$gpuSerial[1]+500000000, y=h, label= paste("Median",round(h,2)), size=2.5, vjust=1.5)
# Seems to be a distinct clustering of mean rendering times into approximately 2 clusters around the mean (41.3s)
min(comp_gpu$gpuSerial)
max(comp_gpu$gpuSerial)
filter(comp_gpu, gpuSerial < (320118118607+500000000)
)
p24 = ggplot(comp_gpu) + geom_point(aes(gpuSerial, avg_dur)) + labs(x = 'GPU S/N', y = 'Mean Render Execution Time (s)') +
geom_hline(yintercept = h, color = 'red', linetype = 'dotdash') +
annotate("text", x=comp_gpu$gpuSerial[1]+500000000, y=h, label= paste("Median",round(h,2)), size=2.5, vjust=1.5)
cache('p24')
c1 = filter(comp_gpu, gpuSerial < (320118118607+500000000))
View(c1)
c1 = filter(comp_gpu, gpuSerial < (320218055257))
c1 = filter(comp_gpu$gpuSerial, gpuSerial < (320218055257))
c1 = comp_gpu$gpuSerial[comp$gpuSerial < 320218055257]
c1 = comp_gpu$gpuSerial[comp_gpu$gpuSerial < 320218055257]
c2 = comp_gpu$gpuSerial[comp_gpu$gpuSerial < 322000000000]
c2 = comp_gpu$gpuSerial[comp_gpu$gpuSerial > max(c1) & comp_gpu$gpuSerial < 322000000000]
c3 = comp_gpu$gpuSerial[comp_gpu$gpuSerial > max(c2) & comp_gpu$gpuSerial < 323500000000]
c4 = comp_gpu$gpuSerial[comp_gpu$gpuSerial > max(c3) & comp_gpu$gpuSerial < 324000000000]
c1 = comp_gpu$gpuSerial[comp_gpu$gpuSerial < 322000000000]
c2 = comp_gpu$gpuSerial[comp_gpu$gpuSerial > max(c1) & comp_gpu$gpuSerial < 323500000000]
c3 = comp_gpu$gpuSerial[comp_gpu$gpuSerial > max(c2) & comp_gpu$gpuSerial < 324000000000]
c4 = comp_gpu$gpuSerial[comp_gpu$gpuSerial > max(c3)]
c1+c2+c3+c4
length(c1)+length(c2)+length(c3)+length(c4)
str(c1)
summary(c1)
max(c1)
min(c1)
test = data_frame(c1,c2,c3,c4)
test = data_frame(c(min(c1),max(c1)))
View(test)
test = data_frame(c(min(c1),max(c1)),c(min(c2),max(c2)),c(min(c3),max(c3)),c(min(c4),max(c4)))
c_df = data_frame(c('Minimum S/N','Maximum S/N'), c(min(c1),max(c1)),c(min(c2),max(c2)),c(min(c3),max(c3)),c(min(c4),max(c4)))
names(c_df)
names(c_df) = c('Range','Cluster 1', 'Cluster 2','Cluster 3', 'Cluster 4')
View(c_df)
sn_cluster_df = data_frame(c('Minimum S/N','Maximum S/N'), c(min(c1),max(c1)),c(min(c2),max(c2)),c(min(c3),max(c3)),c(min(c4),max(c4)))
names(sn_cluster_df) = c('Range','Cluster 1', 'Cluster 2','Cluster 3', 'Cluster 4')
cache('sn_cluster_df')
names(sn_cluster_df) = c('Series Range','Series 1', 'Series 2','Series 3', 'Series 4')
cache('sn_cluster_df')
class(c1)
class(c3)
c3[1]
c2[1]
format(c3[1], scientific=F)
format(c3, scientific=F)
c3
as.character(c3)
as.character(sn_cluster_df)
test = as.character(sn_cluster_df)
sn_cluster_df$`Series 1` = as.character(sn_cluster_df$`Series 1`)
View(sn_cluster_df)
sn_cluster_df$`Series 1` = as.character(sn_cluster_df$`Series 1`)
sn_cluster_df$`Series 2` = as.character(sn_cluster_df$`Series 2`)
sn_cluster_df$`Series 3` = as.character(sn_cluster_df$`Series 3`)
sn_cluster_df$`Series 4` = as.character(sn_cluster_df$`Series 4`)
cache('sn_cluster_df')
View(sn_cluster_df)
length(c1)
sn_cluster_df = data_frame(c('Minimum S/N','Maximum S/N','n'), c(min(c1),max(c1),length(c1)),c(min(c2),max(c2),length(c2)),c(min(c3),max(c3),length(c3)),c(min(c4),max(c4),length(c4)))
names(sn_cluster_df) = c('Series Range','Series 1', 'Series 2','Series 3', 'Series 4')
sn_cluster_df$`Series 1` = as.character(sn_cluster_df$`Series 1`)
sn_cluster_df$`Series 2` = as.character(sn_cluster_df$`Series 2`)
sn_cluster_df$`Series 3` = as.character(sn_cluster_df$`Series 3`)
sn_cluster_df$`Series 4` = as.character(sn_cluster_df$`Series 4`)
cache('sn_cluster_df')
View(sn_cluster_df)
sn_cluster_df = data_frame(row.names=c('Minimum S/N','Maximum S/N','n'), c(min(c1),max(c1),length(c1)),c(min(c2),max(c2),length(c2)),c(min(c3),max(c3),length(c3)),c(min(c4),max(c4),length(c4)))
names(sn_cluster_df) = c('Series Range','Series 1', 'Series 2','Series 3', 'Series 4')
sn_cluster_df$`Series 1` = as.character(sn_cluster_df$`Series 1`)
sn_cluster_df$`Series 2` = as.character(sn_cluster_df$`Series 2`)
sn_cluster_df$`Series 3` = as.character(sn_cluster_df$`Series 3`)
sn_cluster_df$`Series 4` = as.character(sn_cluster_df$`Series 4`)
cache('sn_cluster_df')
View(sn_cluster_df)
sn_cluster_df = data_frame(row.names=c('Minimum S/N','Maximum S/N','n'), c(min(c1),max(c1),length(c1)),c(min(c2),max(c2),length(c2)),c(min(c3),max(c3),length(c3)),c(min(c4),max(c4),length(c4)))
sn_cluster_df = data_frame(c(min(c1),max(c1),length(c1)),c(min(c2),max(c2),length(c2)),c(min(c3),max(c3),length(c3)),c(min(c4),max(c4),length(c4)), row.names=c('Minimum S/N','Maximum S/N','n'))
View(sn_cluster_df)
sn_cluster_df = data_frame(c(min(c1),max(c1),length(c1)),c(min(c2),max(c2),length(c2)),c(min(c3),max(c3),length(c3)),c(min(c4),max(c4),length(c4)))
row.names(sn_cluster_df) = c('Minimum S/N','Maximum S/N','n')
View(sn_cluster_df)
names(sn_cluster_df) = c('Series 1', 'Series 2','Series 3', 'Series 4')
sn_cluster_df$`Series 1` = as.character(sn_cluster_df$`Series 1`)
sn_cluster_df$`Series 2` = as.character(sn_cluster_df$`Series 2`)
sn_cluster_df$`Series 3` = as.character(sn_cluster_df$`Series 3`)
sn_cluster_df$`Series 4` = as.character(sn_cluster_df$`Series 4`)
cache('sn_cluster_df')
