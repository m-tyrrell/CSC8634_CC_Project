---
title: "Analysis of Terascope Benchmarking Data"
author: "Mark R. Tyrrell"
date: "23/01/2019"
output:
  pdf_document: null
  html_document:
    df_print: paged
header-includes:
- \usepackage{graphicx}
- \usepackage{float}
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
knitr::opts_chunk$set(fig.pos = 'H')
# knitr::kable()
```



```{r load_project, include=FALSE}
library(ProjectTemplate);load.project()

```




```{r, echo=FALSE}

```

## 1. Introduction

 Visualization methods
panoramic images show potential for visualizing multiscale urban data. accessible route to supercomputer visualizations on many low cost devices.
Visual supercomputing
deployed 14 PFlop cloud supercomputer, larger than any GPU HPC system in UK. cost in total for one result (a scaling graph) ~£20,000 using > £10 million computer. Azure improved (from K80 to V100) during the project, we immediately benefited. provides access to performance approx. 20 years ahead of current desktop systems.



## 2. Background

Terapixel images offer an intuitive, accessible way to present information sets to stakeholders, allowing viewers to interactively browse big data across multiple scales. The challenge we addressed here is how to deliver the supercomputer scale resources needed to compute a realistic terapixel visualization of the city of Newcastle upon Tyne and its environmental data as captured by the Newcastle Urban Observatory.

Our solution is a scalable architecture for cloud-based visualization that we can deploy and pay for only as needed. The three key objectives of this work are to: create a supercomputer architecture for scalable visualization using the public cloud; produce a terapixel 3D city visualization supporting daily updates; undertake a rigorous evaluation of cloud supercomputing for compute intensive visualization applications.

We demonstrate that it is feasible to produce a high quality terapixel visualization using a path tracing renderer in under a day using public IaaS cloud GPU nodes. Once generated the terapixel image supports interactive browsing of the city and its data at a range of sensing scales from the whole city to a single desk in a room, accessible across a wide range of thin client devices.

Problem Description

The dataset below was created from application checkpoint and system metric output from the production of a terapixel image. There are a variety of problems which can be addressed using the TeraScope dataset. Each will commence with an exploratory data analysis. Examples of questions you may wish to be able to answer through the EDA process are as follows:

Which event types dominate task runtimes?
What is the interplay between GPU temperature and performance?
What is the interplay between increased power draw and render time?
Can we quantify the variation in computation requirements for particular tiles?
Can we identify particular GPU cards (based on their serial numbers) whose performance differs to other cards? (i.e. perpetually slow cards).
What can we learn about the efficiency of the task scheduling process?
A project may focus entirely on the EDA process, or you may also wish to develop a data product to allow interactive exploration of the data. For example, you could develop a Shiny dashboard to display the results of your analysis into render performance and system operation, displaying timelines of execution based on the application checkpoint data.





## 3. Methodology
Data analysis was conducted using the R programming language and the R Studio IDE, with the report composed in Rmarkdown. The **ProjectTemplate** library was used to organise the project and ensure reproducability. Version control was managed using git and Github. 

An archived folder is available containing the original Rmarkdown .rmd file and git log. The folder is structured for standard data mining projects, including subfolders for data, cache, munge and source files. As the data analysis involves operations which require extended processing time, key objects have been cached as .Rdata files. The objects are called along the progression of the script (and Rmarkdown report), while the original code is included in the munge folder. Detailed instructions for reproducing the analysis are included in the main README file.

The original csv data files are too large for Github at 300+ MB. Therefore they are not included in the archived project folder. A download link can be found in the README file. 

### 3.1 Data Understanding

The data made available for this analysis consists of 3 tabular data files in comma-separated value (.csv) format covering multiple aspects of the terapixel rendering process. The data files are detailed below.

#### 2.1.1 Application Checkpoints (application-checkpoints.csv 111.2MB)
This file contains 660,400 observations over 6 variables detailing application checkpoint events throughout the execution of the render job. 

1. **timestamp**   Timestamp of observation entry
2. **hostname**    Unique hostname of the virtual machine auto-assigned by the Azure batch system (Total unique: 1024)
3. **eventName**   Name of the event occuring within the rendering application. Possible Values:
    + **Tiling**    Tiling is where post processing of the rendered tile is taking place
    + __Saving__    Config Saving Config is simply a measure of configuration overhead
    + __Render__    Render is when the image tile is is being rendered
    + __TotalRender__ TotalRender is the entire task
    + __Uploading__ Uploading is where the output from post processing is uploaded to Azure Blob Storage

4. **eventType** Possible Values: __START__, __STOP__
5. **jobId** ID of the Azure batch job (Total: 3 one for each level 4, 8 and 12)
6. **taskId** ID of the Azure batch task (Total: 65793 level 4 1x1; level 8 16x16; level 12 256x256)

#### 3.1.2 GPU (gpu.csv 208.7MB)
This file contains 1,543,681 observations over 8 variables detailing metrics that were output regarding the status of the GPU on the virtual machine.

1. **timestamp**   Timestamp of observation entry
2. **hostname**    Unique hostname of the virtual machine auto-assigned by the Azure batch system (Total unique: 1024)
3. **gpuSerial** Unique serial number of the physical GPU card (Total unique: 1024)
4. **gpuUUID** Unique system id assigned by the Azure system to the GPU unit (Total unique: 1024)
5. **powerDrawWatt** Power draw of the GPU in watts
6. **gpuTempC** Temperature of the GPU in Celsius
7. **gpuUtilPerc** Percent utilisation of the GPU Core(s)
8. **gpuMemUtilPerc** Percent utilisation of the GPU memory

#### 3.1.3 Task X Y (task-x-y.csv 6.2MB)
This file contains 65,793 observations over 5 variables detailing the tile x,y co-ordinates of which part the image was being rendered for each task.

1. **jobId** ID of the Azure batch job (Total: 3 one for each level 4, 8 and 12)
2. **taskId** ID of the Azure batch task (Total: 65793 level 4 1x1; level 8 16x16; level 12 256x256)
3. **x** X co-ordinate of the image tile being rendered (Total unique: 256)
4. **y** Y co-ordinate of the image tile being rendered (Total unique: 256)
5. **level** The visualisation created is a zoomable "google maps style" map. In total we create 12 levels. Level 1 is zoomed right out and level 12 is zoomed right in. You will only see levels 4, 8 and 12 in the data as the intermediate level are derived in the tiling process.


### 3.2 Data Preparation

The csv files were read into R and reviewed. Data types were changed as required (eg. timestamps, factors to char etc). In the case of the GPU dataset, the extraneous column **gpuUUID** was dropped, as it was redundant with **gpuSerial** as a unique identifier. The Task X Y dataset was fully merged into the Application Checkpoint dataset, as it contained only 3 unique columns. The resulting object **app_task** was cached along with the **gpu** object for the GPU dataset, after checking for NAs. These cached objects form the basis of the analysis.

The analysis made heavy use of the Dplyr package to transform and filter **app_task** and **gpu**. Multiple analyses were based on transformed datasets that required extensive processing time. These datasets were cached as objects, with the original code included in the munge folder. This function was particularly exemplified in the form of the **gpu_task** dataset, which was the output of the computation of mean resource usage for each event under each of the 65793 level 4 tasks in the **app_task** dataset. This computation took three hours on a Corei5 system.


## 4 Exploratory Data Analysis

As detailed in section 2.2, the data show 1024 unique VMs, each with an attached GPU. These 1024 GPUs were used to render 65,793 unique tiling jobs identified by a unique task number. 

The performance data over 10 minutes for a sample GPU (S/N: 323617042631) is displayed in Figure 1. The cycles for each tiling job can be clearly seen in the plot for each metric. Each tiling job took appears to take approximately 40 seconds.


```{r eda_01, fig.cap = "Sample GPU Performance Metrics over Time", fig.pos = "H", out.width='80%', fig.asp=0.75, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE, results= 'hide', , cache=TRUE}
# Create index column with accessible numbers for each gpu serial number
gpu_id = data.frame(gpuSerial = sort(unique(gpu$gpuSerial)), id =  1:1024)
# Create a copy of gpu dataset and insert gpu index vector
gpu_r = gpu
gpu_r = left_join(gpu_r, gpu_id)

# Filter gpu_r dataset by unique gpu index number for plotting
gpu_t = gpu_r %>%
        filter(id == 500) %>%
        arrange(timestamp) %>%
        filter(row_number() < 300)

# Plot each metric in the subsetted gpu_r dataset
p1 = ggplot(gpu_t, aes(timestamp, powerDrawWatt)) + geom_line(color='#0066CC') + labs( x = 'timestamp (s)', y = 'Power (W)')
p2 = ggplot(gpu_t, aes(timestamp, gpuTempC)) + geom_line(color='#0066CC') + labs(x = 'timestamp (s)', y = 'Temperature (C)')
p3 = ggplot(gpu_t, aes(timestamp, gpuUtilPerc)) + geom_line(color='#0066CC') + labs(x = 'timestamp (s)', y = 'GPU Usage (%)')
p4 = ggplot(gpu_t, aes(timestamp, gpuMemUtilPerc)) + geom_line(color='#0066CC') + labs(x = 'timestamp (s)', y = 'Memory Usage (%)')
# Plot grid of 4
grid.arrange(p1, p2, p3, p4, ncol=2)
```


The 



```{r eda_02, fig.cap = "GPU Performance Metrics Correlation", fig.pos = "H", out.width='80%', fig.asp=0.75, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE, results= 'hide', cache=TRUE}
# Q2 interplay between GPU Performance metrics
q2 = gpu %>%
        select(4:7)

# Take sample of q2 index
x = sample(1:dim(q2)[1],10000)
# Slice q2 by sample
q2_samp = q2[x,]

p5 = ggplot(q2_samp, aes(powerDrawWatt, gpuTempC)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Power (W)', y = 'Temperature (C)')
p6 = ggplot(q2_samp, aes(gpuUtilPerc, gpuTempC)) + geom_point(size=0.25) + stat_smooth()+ labs( x = 'GPU Usage (%)', y = 'Temperature (C)')
p7 = ggplot(q2_samp, aes(gpuUtilPerc, powerDrawWatt)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'GPU Usage (%)', y = 'Power (W)')
p8 = ggplot(q2_samp, aes(gpuUtilPerc, gpuMemUtilPerc)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'GPU Usage (%)', y = 'Memory Usage (%)')
p9 = ggplot(q2_samp, aes(powerDrawWatt, gpuMemUtilPerc)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Power (W)', y = 'Memory Usage (%)')
p10 = ggplot(q2_samp, aes(gpuTempC, gpuMemUtilPerc)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Temperature (C)', y = 'Memory Usage (%)')

# Plot grid of 6
grid.arrange(p5, p6, p7, p8, p9, p10, ncol=3)
```



The learners are highly educated overall, with almost 66 percent reporting a minimum of a bachelor degree. 

```{r eda_03, echo=FALSE}
# Q1 Which tasks dominate runtimes?
task_runtime_means = task_runtimes %>%
        # Aggregate by event taking mean of all observations for each event
        group_by(eventName) %>%
        summarise('mean duration (s)' = mean(duration), n = n()) %>%
        rename('Event Name' = eventName)
# Round runtime        
task_runtime_means$`mean duration (s)` = round(task_runtime_means$`mean duration (s)`,2)

knitr::kable(task_runtime_means, booktabs = TRUE, caption = 'Task Runtime Mean Durations (Level 12)')
```




```{r eda_04, fig.cap = "Distribution by Execution Time: Render \\& Tiling", fig.pos = "H", out.width='80%', fig.asp=0.75, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE, results= 'hide', , cache=TRUE}
##### Execution Time Plot - histograms showing distribution of execution times for each event type (Render & Tiling)
grid.arrange(p12, p13, ncol=1)
```




```{r eda_05, fig.cap = "Distribution by Execution Time: Uploading \\& Save Config", fig.pos = "H", out.width='80%', fig.asp=0.75, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE, results= 'hide', , cache=TRUE}
##### Execution Time Plot - histograms showing distribution of execution times for each event type (Render & Tiling)
grid.arrange(p14, p15, ncol=1)
```

The geographical heat map presented in Figure 1 provides a wider view of the student base by nationality. The countries highlighted on the map represent the top 50 countries by number of students. Although 200 countries in total are represented in the learner nationality dataset, the top 50 countries account for 90 percent of the total. 
```{r eda_06, fig.cap = "Distribution by Power Consumption: Render \\& Tiling", fig.pos = "H", out.width='80%', fig.asp=0.75, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE, results= 'hide', , cache=TRUE}
grid.arrange(p16, p17, ncol=1)
```

```{r eda_07, fig.cap = "Distribution by Power Consumption: Uploading \\& Save Config", fig.pos = "H", out.width='80%', fig.asp=0.75, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE, results= 'hide', , cache=TRUE}
grid.arrange(p18, p19, ncol=1)
```


```{r eda_08, fig.cap = "Distribution by Temperature: Render \\& Tiling", fig.pos = "H", out.width='80%', fig.asp=0.75, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE, results= 'hide', cache=TRUE}
grid.arrange(p20, p21, ncol=1)
```

```{r eda_09, fig.cap = "Distribution by GPU \\& Memory: Render", fig.pos = "H", out.width='100%', fig.asp=0.75, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE, results= 'hide', cache=TRUE}
grid.arrange(p22, p23, ncol=1)
grid.arrange(p20,p21,p22, p23, ncol=2)
```


```{r}
knitr::include_graphics(path = "../graphs/p24.pdf")
```






### 3.2 Evaluate Course Completion Performance Metrics

Measurement of student performance is critical in any education system, even in a largely cost-free online course. Poor performance can reflect poorly on the student satisfaction, and therefore completion rate. The enrollment dataset contains numerous interesting data in this regard.


#### 3.2.1 Course Completion Metrics

Of the 37,296 students who enrolled, only 2154 completed the course. This results in a completion rate of 5.8 percent. While low, this rate fits closely with the standard completion rate for MOOCs of 5.5 percent[2]. As the barrier for entrance is small, MOOCs attract a great deal of prospective students. 

The enrollment dataset contains a date for enrollment and a separate date for course completion. From this, the number of days taken to complete the course were derived. A histogram of these data is presented in Figure 2. 

At registration, the course is estimated to take three weeks to complete at an input level of three hours per week. The data show that this is not the case. According to the enrollments data the median days to completion was 40. The mean was 71, a skew to the right clearly represented in Figure 2. However, it must be noted that the data source for this metric encompasses total time between enrollment and completion, rather than the actual time necessary to complete each section of the course (ref. Section 3.2.2).

It is interesting to note the bi-modal nature of the distribution, with approximately 15 percent of students completing the course around the 125 day mark. This may be the result of auto-scheduled email prompts or other such communication.

```{r eda_10, fig.cap = "Distribution by CPU \\& Memory: Render", fig.pos = "H", out.width='80%', fig.asp=0.75, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE, results= 'hide'}
# ##### Completion Data (n = 2154)

```




#### 3.2.2 Course Step Duration

The step activity dataset provides a record of the 23 course steps completed by each user, including the start and finish dates. From these data, the number of days taken to complete the step were derived. A column chart of these data is presented in Figure 3, showing the average duration for completion of each step. For clarity, the sub-steps between the following sections were removed from the chart:

* step 1.11 through step 1.19
* step 2.11 through step 2.19
* step 2.21 through step 2.29
* step 3.11 through step 3.19

The sum of the average durations for each section displayed in Figure 3 is 25.8 days. However, with the filtered subs-steps added, the total mean is 42.9 days. As with the findings in section 3.2.1, this figure is in excess of the three weeks completion estimate provided at course registration.  

As demonstrated by the chart, section one takes longer for users to complete on average, with a notable downward trend for each section as the course progresses. This trend is with the exception of the outlier (step 3.21) which at approximately 8 days, took significantly longer on average for the students to complete. It is unclear from this analysis why this particular step took so long to complete.

```{r comp_duration, fig.cap = "Average Completion Duration per Step (days): Section 1.0", fig.pos = "H", out.width='80%', fig.asp=.75, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE, results= 'hide'}
# #Plot average days per step
# ggplot(df_step_avg, aes(step, total_days)) + geom_col(size = 1, fill=I("#0066CC"), colour = "#0066CC", alpha=I(0.8)) + 
#         labs(x = "Step #", y = "Days")  + 
#         scale_x_continuous(breaks = round(seq(min(df_step_avg$step), max(df_step_avg$step), by = 0.1),1))
```


#### 3.2.3 Course Dropout Stage

The inclusion of a unique identifier (learner_id) in the step activity dataset allowed for analysis of the maximum step completed by each learner. As with section 3.2.2, the sub-steps were removed from the analysis for clarity. The findings are presented as a histogram in Figure 4. The distribution count across each step closely follows the average step completion duration data outlined in section 3.3.2, including the outlier (step 3.21), but with the exception of the initial step (1.1). Fully 32 percent of learners fail to continue the course after step 1.1. 

```{r dropouts, fig.cap = "Dropouts: Last Step Completed", fig.pos = "H", out.width='80%', fig.asp=0.75, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE, results= 'hide'}
#Plot frequency of drop out by step number
# ggplot(df_step_drop, aes(step)) + geom_histogram(binwidth = 0.1, fill=I("#0066CC"), col=I("white"), alpha=I(0.8)) + 
#         labs(x = "Step #", y = "Count (# of Learners)") + theme(text = element_text(size=8)) +
#         scale_x_continuous(breaks = round(seq(min(df_step_drop$step), max(df_step_drop$step), by = 0.1),1))
# # ggsave(file.path('graphs', 'step_drop.pdf'))
```




### 3.3 Identify Demographic Predictors of Course Completion

As stated in section 3.1, submission of demographic information for learners is voluntary. As such, only approximately 10 percent of observations contained complete demographic data. However, this resulted in a dataset containing 3819 observations. Assuming the population distribution is normal, and the process for obtaining the data was random (i.e. learners who submitted demographic data were not skewed toward a specific subset), the sample size is large enough to meet the conditions for inference. Therefore substantive conclusions can be made about the overall learner population from this sample with reasonable confidence. 

In the following sections, the filtered dataset is analysed by subset, in order to form hypotheses about predictors of success based on demographic data. 

#### 3.3.1 Performance by Gender

The data demonstrated in Figure 5 a) show a pronounced tendency for male students to achieve higher step progress than their female counterparts. The difference in medians is fully half a section higher, with a less distinct difference in interquartile range (IQR). Though this finding would seem to support common stereotypes of gender roles, the gap in achievement is notable in that it is significantly less than the overall trend. A recent study in the United States[3] found that women were vastly underrepresented in science, technology, engineering and mathematics (STEM) fields, at only 29 percent. These findings are therefore encouraging.



#### 3.3.2 Performance by Age Range

Perhaps surprisingly for an IT-focused course, the data show a marked progression in step completion with increasing age ranges. Referring to Figure 5 b), the ranges from age 56 upwards demonstrate a clear proclivity for older people to achieve higher completion rates with the course. The median for both ranges is greater than section three; a marked contrast to the medians for the first three age ranges, which fail to complete section one. 


```{r pred-vars, fig.cap = "Step Completion by Variable", fig.pos = "H", out.width='100%', fig.asp=1.1, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE, results= 'hide'}
# grid.arrange(p1, p2, p3, p4, ncol=2)
```


#### 3.3.3 Performance by Education

The determinants of appear to be somewhat correlated with step completion outcomes for the course. Referring to Figure 5 c), there is notably upwards trend from less-than secondary level, through post-graduate qualifications. This analysis assumes that the "professional" education level indicates some type of professional-level certification. Though the apprentice level appears to counter this trend, the sample size is very small (n = 13) and can therefore not necessarily meaningful.



#### 3.3.4 Performance by Employment Status

The major inference apparent in Figure 5 d) is that retired individuals appear to be significantly more likely to get farther in the course, achieving a median of completion of step 3.2. This finding correlates almost exactly with Figure 5 c) (ref. 3.3.3), in that the median step completion for people of retirement age (> 65) was approximately step 3.2. 


#### 3.3.5 Performance by Country

There are no stand-out conclusions to be drawn by Figure 6. No single country appears to perform significantly better than others. However it is notable that of the top 20 countries by step completion, 11 are English speaking or have large English-speaking populations. 

```{r pred-country, fig.cap = "Average Step Completion by Country (Top 20)", fig.pos = "H", out.width='80%', fig.asp=0.75, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE, results= 'hide'}
####Predictor Country
# ggplot(data = df_step_perf_ct, mapping = aes(x = reorder(detected_country, step), y = step)) + geom_col(fill=I("#0066CC"), col=I("white"), alpha=I(0.8)) +
#         theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5)) + 
#         labs(x = "Country", y = "Step", title = "")
```




### 3.4 Identify Factors Contributing to Paid Upgrades

The working dataset contains 37,296 observations, however only 289 of these individuals paid for the course certification. Unfortunately the subset of these observations for which full demographic data were available is insufficient for statistical inference (n = 13). Therefore, no meaningful conclusions can be made from analysis of these data with regard to identifying predictors of purchase conversions. 

The data do appear to show that conversely, certification purchases drive course completion. As demonstrated by Table 6, 81 percent of learners who paid for certification had finished the course. Of this group (n = 233), 64 percent had purchased the course prior to completion (ref. Table 7). This finding indicates that per-completion purchases are a major motivator towards course completion, in that students who have invested financially in the course are more likely to see it through to completion. 
Of the subset of students who completed the course, then paid after completion (n = 84), 61 percent paid the day of completion, while 75 percent had paid within one week. This finding possibly reflects automated email reminders to graduates, and should be compared against platform standard practices to measure efficacy. 

```{r, echo=FALSE}
# knitr::kable(tblFun(df_enrol_paid$completed), booktabs = TRUE, caption = 'Purchases by Course Completion (n = 289)')
# knitr::kable(tblFun(df_enrol_paid$purchased_cert_bin), booktabs = TRUE, caption = 'Purchases by Time (completed only; n = 233)')
```


### 3.5 Model Predictors of Course Completion

Classification analysis of the the demographic predictors of course completion success was accomplished using a logistic regression model. The results of this analysis identified minimal contribution to model predictive functionality from higher education and employment status. The major predictor was age group, with gender also making a notable contribution. These findings are mostly in keeping with the findings identified in section 3.3. With the exception of 'retired' employment status and a marginal trend upwards for step performance compared to educational achievement, the major correlations identified were indeed gender and age group. 

A summary of the reduced model is provided below, after removing education and employment status. The model test error was computed using k=10 folds cross validation. The resulting calculated error is 13 percent. However, as demonstrated by the confusion matrix, the model is rather pessimistic. For all 382 test observations, the model predicted probabilities for zero course completions. This result is unlikely of course, as 13 percent of observations in the sample had completed the course. Further evaluation using additional data could confirm the viability of the model.


```{r, echo=TRUE}
# # Display summary of model parameters
# summary(lr_fit_final)
# # Model Confusion Matrix
# conf_matrix
# # Model Test Error
# lr_test_error
# # Cross Validated Test Error (k = 10)
# cross_val_test_error
```



## 4. Summary of Results Evaluation and Business Conclusions

The analysis returned numerous results relevant to the business objectives. These results are summarised and evaluated below. Summary conclusions and reccomendations are included where applicable. 

### 4.1 Increase Enrolment

**Gender Gap**\newline
The difference in course enrollment between genders demonstrates that there is room for growth in female student usage. It may be possible to boost female enrollment by differential marketing strategies or tailoring the VLE in a gender-focused way.

**Overly Educated Student Base**\newline
There is room for growth in lesser educated demographics

**Largely UK Focused**\newline
There is opportunity for student base expansion to English-speaking developing countries. Review of platform multi-language options could provide insights into how non-english audiences could be better reached.


### 4.2 Improve Course Completion Rate

**Completion Rate**\newline 
At 5.8 percent, the completion rate is standard performance for the sector, but lessons learned by other MOOCs could be applied here to increase completion rates. For instance, is the platform providing the latest in terms of user interface and pedagocical approaches?

**Course Duration**\newline
The 71 day median (43 day mean) for course completion contrasts with the course estimate of 3 weeks. It is possible students sign up expecting an easier or quicker course. The stated completion estimate should be reviewed and either changed, or course structure change to more accurately reflect the real course duration.

**Day 90 Course Completion Upswing**\newline
The bimodal nature of the dropout by step findings raises questions about why there is such a notable increase in course completions from the 3-month mark. Is it in fact auto-scheduled email reminders? This should be reviewed as it would prove efficacy of this functionality and provide an evidence base for other such automated reminders.

**Step 3.21 (Duration Uutlier)**\newline 
Taking on average 8 days to complete, this step was a major outlier. The particulars of this step should be reviewed and the step possibly reformatted, as it correlates with a high dropout for the same step number.

**Downward Trend in Dropout Follows Step Durations**\newline 
Though this could be simply a platform-based learning curve, it could signify a less than optimum structuring. For instance,  easier steps earlier in the course might result in greater step achievement and course completion on average.

**Gender Performance Trend**\newline
Review gender-focused course accessibility for females. Why are women doing worse than men? It may be possible to tailor the VLE in a gender-focused way based on provided demographic student data.

**Age Performance Trend**\newline
Review course age-focused attributes. For instance, could a different interface for younger learners improve performance?

**Retirees Performance Trend**\newline 
Retirees show above average completion rates. Marketing to this demographic could improve overall course completion rates.

**Nationality Performance Trend**\newline
Review platform multi-language options as well as current VLE user interface with regards to english as a second language (ESL) users. 


### 4.3 Increase Percentage of Paid Upgrades

**Course Purchase Correlation with Completion**\newline
Increased data from future cohorts could further elucidate the dynamics behind these metrics. A review of causation could be possible with an online-based (operational) randomised control trial (RCT). This could involve offering free/discounted evaluation/certification to random learners and comparing results with a control group.


## Conclusions for future data mining

The large quantity of missing demographic data (approximately 90%) limited meaningful analysis of subsets of the sample. For instance, though there were 289 observations in the dataset of paid learners, only 13 of these obervations had complete demographic variables. Therefore it was impossible to draw statistical inference about demographic factors contributing to paid upgrades. If there was someway of enticing users to enter demographic data more frequently, it could vastly increase the utitlity of the learning analytics for this course. 



## References

1.  http://www.policyconnect.org. uk/hec/sites/site_hec/files/ report/419/fieldreportdownload/ frombrickstoclicks- hecreportforweb.pdf

2. Chuang, Isaac and Ho, Andrew, HarvardX and MITx: Four Years of Open Online Courses -- Fall 2012-Summer 2016 (December 23, 2016). Available at SSRN: https://ssrn.com/abstract=2889436 or http://dx.doi.org/10.2139/ssrn.2889436

3. National Science Foundation, National Center for Science and Engineering Statistics, Scientists and Engineers Statistical Data System (SESTAT) and National Survey of College Graduates (NSCG) (1993, 2013), http://sestat.nsf.gov.

## Appendix: R-Code

#### The code below is provided for reference and reproducibility purposes. The sections detail the individual data files used for the analysis, and the order reflects the loading seqence. 

```{r, echo=TRUE, eval=FALSE}
################################### Munge: 01-A.R ########################################

##### Read csv data files, combine by type (rbind), clean/transform as required and cache as .RData

##### Enrolments
# Load and combine all datasets
list_sel = list.files(path = "data/", pattern = "*enrolments*")
x <- lapply(list_sel, function(i) read.csv(file = paste("data/",i,sep = "")))
df_enrolments <- do.call('rbind', x) 

# Clean & Transform
df_enrolments = df_enrolments %>%
        #Convert date columns to POSIX UTC
        mutate(enrolled_at = as.character(enrolled_at)) %>%
        mutate(unenrolled_at = as.character(unenrolled_at)) %>%
        mutate(fully_participated_at = as.character(fully_participated_at)) %>%
        mutate(purchased_statement_at = as.character(purchased_statement_at)) %>%
        mutate(enrolled_at = ymd_hms(enrolled_at, tz="UTC")) %>%
        mutate(unenrolled_at = ymd_hms(unenrolled_at, tz="UTC")) %>%
        mutate(fully_participated_at = ymd_hms(fully_participated_at, tz="UTC")) %>%
        mutate(purchased_statement_at = ymd_hms(purchased_statement_at, tz="UTC")) %>%
        #Convert country to character vector
        mutate(detected_country = as.character(detected_country)) %>%
        # Add derived variables to display Days Enrolled, Days to Completion, and binary completion variable
        mutate(days_enrolled = as.numeric(round(difftime(unenrolled_at, enrolled_at, unit="days"),0))) %>%
        mutate(days_to_completion = as.numeric(round(difftime(fully_participated_at, enrolled_at, unit="days"),0))) %>%
        mutate(completed = ifelse(is.na(fully_participated_at), 0, 1)) %>%
        mutate(learner_id = as.character(learner_id))

cache("df_enrolments")

##### Step Activity
# Load and combine all datasets
list_sel = list.files(path = "data/", pattern = "*activity*")
x <- lapply(list_sel, function(i) read.csv(file = paste("data/",i,sep = "")))
df_step_activity <- do.call('rbind', x)

# Clean & Transform
df_step_activity = df_step_activity %>%
        #Convert date columns to POSIX UTC
        mutate(first_visited_at = ymd_hms(first_visited_at, tz="UTC")) %>%
        mutate(last_completed_at = ymd_hms(last_completed_at, tz="UTC")) %>%
        mutate(learner_id = as.character(learner_id))

cache("df_step_activity")


##### Functions
#Tabulate function (shows percentage)
tblFun <- function(x){
        tbl <- table(x)
        res <- cbind(tbl,round(prop.table(tbl)*100,2))
        colnames(res) <- c('Count','Percentage')
        res
}

cache("tblFun")

############################### EDA EDA_Enrolment.R ######################################

# DF Step Activity

# Create working object from cached dateset
df_enrol = df_enrolments

cache("df_enrol")


### 3.1 Profile existing student base

#Student data summary statistics (filtering out unknowns and insufficient samples)
df_enrol_dt = filter(df_enrol, gender != "Unknown" & gender != "nonbinary" & gender != "other" 
                     & age_range != "Unknown" & detected_country != "--" & highest_education_level != "Unknown" & employment_status != "Unknown")

#Reorder factor levels and remove unknown
df_enrol_dt$age_range = factor(df_enrol_dt$age_range, c("<18","18-25","26-35","36-45","46-55","56-65",">65"))
df_enrol_dt$gender = factor(df_enrol_dt$gender, c("male", "female"))
df_enrol_dt$highest_education_level = factor(df_enrol_dt$highest_education_level, c("apprenticeship","less_than_secondary","professional","secondary","tertiary","university_degree","university_masters","university_doctorate"))
df_enrol_dt$employment_status = factor(df_enrol_dt$employment_status, c("full_time_student","looking_for_work","not_working","retired","self_employed","unemployed","working_full_time","working_part_time"))
#Rename factor levels
df_enrol_dt$highest_education_level = mapvalues(df_enrol_dt$highest_education_level, 
        from = c("apprenticeship","less_than_secondary","professional","secondary","tertiary","university_degree","university_masters","university_doctorate"), 
        to = c("apprentice","< secondary","professional","secondary","tertiary","bachelors","masters","doctorate"))
df_enrol_dt$employment_status = mapvalues(df_enrol_dt$employment_status,
        from = c("full_time_student","looking_for_work","not_working","retired","self_employed","unemployed","working_full_time","working_part_time"),
        to = c("student","seeking work","not working","retired","self-employed","unemployed","full-time work","part-time work"))


cache("df_enrol_dt")


#Gender histogram
ggplot(df_enrol_dt) + stat_count(aes(gender), fill=I("#0066CC"), col=I("white"), alpha=I(0.8)) +
        labs(x = "Gender", y = "Count", title = "Gender")

tblFun(df_enrol_dt$gender)

#Age group histogram
ggplot(df_enrol_dt) + stat_count(aes(age_range), fill=I("#0066CC"), col=I("white"), alpha=I(0.8)) +
        labs(x = "Age Groups", y = "Count", title = "Age Groups")

tblFun(df_enrol_dt$age_range)

#Education histogram
ggplot(df_enrol_dt) + stat_count(aes(highest_education_level), fill=I("#0066CC"), col=I("white"), alpha=I(0.8)) +
        labs(x = "Education", y = "Count", title = "Education")

tblFun(df_enrol_dt$highest_education_level)

#Employment histogram
ggplot(df_enrol_dt) + stat_count(aes(employment_status), fill=I("#0066CC"), col=I("white"), alpha=I(0.8)) +
        labs(x = "Employment Status", y = "Count", title = "Employment Status")

tblFun(df_enrol_dt$employment_status)

#Country
#Transform data for detected country, aggregate by count for each country, select top countries by x amount
df_enrol_ct = df_enrol %>%
        dplyr::select(detected_country) %>%
        filter(detected_country != "--") %>%
        mutate(count = 1) %>%
        group_by(detected_country) %>%
        summarise(sum(count)) %>%
        rename(Country = detected_country, Count = "sum(count)") %>%
        mutate(Percentage = round(Count/sum(Count)*100,2)) %>%
        filter(Count > 95) %>%
        arrange(desc(Count))

cache("df_enrol_ct")

#Convert to names for table
df_enrol_ct_name = df_enrol_ct
df_enrol_ct_name$Country = countrycode(df_enrol_ct_name$Country, "iso2c", "country.name")
cache("df_enrol_ct_name")

#Plot and table
ggplot(df_enrol_ct_name, aes(Country, Count)) + geom_col(fill=I("#0066CC"), col=I("white"), alpha=I(0.8)) + 
        labs(x = "Country", y = "Count", title = "Nationality")

df_enrol_ct_name[1:10,]

#Create map object        
map_country = invisible(joinCountryData2Map(df_enrol_ct, joinCode = "ISO2", nameJoinColumn = "Country", verbose = FALSE))
cache("map_country")
#creating a user defined colour palette
# op = palette(c('green','yellow','orange','red'))
# #find quartile breaks
# cutVector = quantile(df_enrol_ct$Count, prob = seq(0, 1, length = 11), type = 5)
# #classify the data to a factor
# df_enrol_ct$Count = cut(df_enrol_ct$Count, cutVector, include.lowest = TRUE)
# #rename the categories
# levels(df_enrol_ct$Count) = c('low', 'med', 'high', 'vhigh', 't1', 't2','t3','t4','t5','t6')
# #mapping by shade
# par(mai=c(0,0,0.2,0),xaxs="i",yaxs="i")
mapCountryData(map_country, nameColumnToPlot = "Count", catMethod='logFixedWidth', mapTitle='Learners by Top 50 Countries', colourPalette = "heat", oceanCol='lightblue', missingCountryCol='white', aspect = 1.5)

#mapping by bubble
# par(mai=c(0,0,0.2,0),xaxs="i",yaxs="i")
# mapBubbles(dF=map_country, nameZSize="Count", nameZColour="Count", numCats = 10, catMethod='categorical', colourPalette='white2Black', oceanCol='lightblue', landCol='wheat', addColourLegend = FALSE, mapTitle='Test')

### 3.2 Performance Metrics

##### Completion Data (n = 2154)
df_enrol_days_cp = filter(df_enrol, !is.na(days_to_completion))

cache("df_enrol_days_cp")

#Plot Course Completion Duration frequency
ggplot(df_enrol_days_cp, aes(days_to_completion)) + geom_histogram(binwidth = 10, fill=I("#0066CC"), col=I("white"), alpha=I(0.8)) + 
        labs(x = "Days", y = "Count", title = "Histogram: Successful Course Completion Duration (days)") + theme(text = element_text(size=9)) +
        scale_x_continuous(breaks = round(seq(min(df_enrol_days_cp$days_to_completion), max(df_enrol_days_cp$days_to_completion), by = 25),1))
ggsave(file.path('graphs', 'course_completion_duration.pdf'))


############################### EDA EDA_Step_Activity.R ##################################

#DF Step Activity

# Create working object from cached dateset
df_step = df_step_activity

# 1. Analysis of Completion Duration by Step
#Compute number of average days take to complete each step
df_step_avg = df_step %>%
        #Add derived variable for duration per step
        mutate(total_days = as.numeric(round(difftime(last_completed_at, first_visited_at, unit="days"),0))) %>%
        #Strip all variables except step and total_days
        dplyr::select(step, total_days) %>%
        #Aggregate
        group_by(step) %>%
        summarise(total_days = round(mean(total_days, na.rm = TRUE),1)) %>%
        #Remove sub-levels
        filter(step < 1.11 | step > 1.19) %>%
        filter(step < 2.11 | step > 2.19) %>%
        filter(step < 2.21 | step > 2.29) %>%
        filter(step < 3.11 | step > 3.19)

cache("df_step_avg")

#Plot average days per step
ggplot(df_step_avg, aes(step, total_days)) + geom_col(size = 1, fill=I("#0066CC"), colour = "#0066CC", alpha=I(0.8)) + labs(x = "Step", y = "Days", 
        title = "Average Completion Duration per Step (days)")  + 
        scale_x_continuous(breaks = round(seq(min(df_step_avg$step), max(df_step_avg$step), by = 0.1),1))
ggsave(file.path('graphs', 'avg_comp_duration_step.pdf'))


# 2. Analysis of Dropout Frequency by Step
df_step_max = df_step %>%
        #Compute maximum level completed for each unique learner ID
        filter(is.na(last_completed_at)) %>%
        dplyr::select(learner_id, step) %>%
        group_by(learner_id) %>%
        summarise(step = max(step))

cache("df_step_max")

df_step_drop = df_step_max %>%
        # Add Count variable (= 1 for each row) and drop learner_id
        mutate(Count = ifelse(!is.na(learner_id), 1, 0)) %>%
        mutate(learner_id = NULL) %>%
        # Remove sub-levels
        filter(step < 1.11 | step > 1.19) %>%
        filter(step < 2.11 | step > 2.19) %>%
        filter(step < 2.21 | step > 2.29) %>%
        filter(step < 3.11 | step > 3.19)
        
cache("df_step_drop")
 
#Plot frequency of drop out by step number
ggplot(df_step_drop, aes(step)) + geom_histogram(binwidth = 0.1, fill=I("#0066CC"), col=I("white"), alpha=I(0.8)) + 
        labs(x = "Step", y = "Count", title = "Histogram: Last Step Completed") +
        scale_x_continuous(breaks = round(seq(min(df_step_drop$step), max(df_step_drop$step), by = 0.1),1)) +
        theme(text = element_text(size=10)) 
ggsave(file.path('graphs', 'dropout_by_step.pdf'))

#Percentage dropout by step 1.1
dim(filter(df_step_drop, step == 1.1))[1]/dim(df_step_drop)[1]


################################# EDA EDA_Join.R #########################################


### Join enrolments data (age, gender etc) with step activity on learner_id and look at performance based on demographics etc (n= 3995)
# Left join on enrolements data (as opposed to step activity data) because demographic (interesting) data is complete. 
#Dataset containing full demographic variables (n = 3819)
df_step_perf = left_join(df_enrol_dt, df_step_max, copy = FALSE)
df_step_perf = df_step_perf %>%
        #Drop unused columns
        dplyr::select(-(enrolled_at:fully_participated_at)) %>%
        mutate(country = NULL) %>%
        #Convert all "completed" steps to 3.9 (otherwise shows NA in some cases)
        mutate(step = ifelse(is.na(step) & completed == 1, 3.9, step)) %>%
        #Create new binary var to show course purchase
        mutate(purchased = ifelse(!is.na(purchased_statement_at),1,0))

cache("df_step_perf")

####Predictor Gender
p1 = ggplot(data = df_step_perf, mapping = aes(x = gender, y = step)) + geom_boxplot(fill=I("#0066CC"), col=I("darkgrey"), alpha=I(0.8), size=0.7) + 
        labs(x = "Gender", y = "Step", title = "a) Gender") + 
        scale_y_continuous(breaks = round(seq(min(df_step_perf$step, na.rm = TRUE), max(df_step_perf$step, na.rm = TRUE), by = 0.2),1))
dat1 = ggplot_build(p1)$data[[1]]
p1 = p1 + geom_segment(data=dat1, aes(x=xmin, xend=xmax, y=middle, yend=middle), colour="white", size=0.7)
cache("p1")

####Predictor Age
p2 = ggplot(data = df_step_perf, mapping = aes(x = age_range, y = step)) + geom_boxplot(fill=I("#0066CC"), col=I("darkgrey"), alpha=I(0.8), size=0.7) + 
        labs(x = "Age Range", y = "Step", title = "b) Age Range") +
        scale_y_continuous(breaks = round(seq(min(df_step_perf$step, na.rm = TRUE), max(df_step_perf$step, na.rm = TRUE), by = 0.2),1))
dat2 = ggplot_build(p2)$data[[1]]
p2 = p2 + geom_segment(data=dat2, aes(x=xmin, xend=xmax, y=middle, yend=middle), colour="white", size=0.7)
cache("p2")

####Predictor Education
p3 = ggplot(data = df_step_perf, mapping = aes(x = highest_education_level, y = step)) + geom_boxplot(fill=I("#0066CC"), col=I("darkgrey"), alpha=I(0.8), size=0.7) + 
        labs(x = "Education Level", y = "Step", title = "c) Education") +
        scale_y_continuous(breaks = round(seq(min(df_step_perf$step, na.rm = TRUE), max(df_step_perf$step, na.rm = TRUE), by = 0.2),1)) +
        theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
dat3 = ggplot_build(p3)$data[[1]]
p3 = p3 + geom_segment(data=dat3, aes(x=xmin, xend=xmax, y=middle, yend=middle), colour="white", size=0.7)
cache("p3")

####Predictor Employment Status
p4 = ggplot(data = df_step_perf, mapping = aes(x = employment_status, y = step)) + geom_boxplot(fill=I("#0066CC"), col=I("darkgrey"), alpha=I(0.8), size=0.7) + 
        labs(x = "Employment Status", y = "Step", title = "d) Employment Status") +
        scale_y_continuous(breaks = round(seq(min(df_step_perf$step, na.rm = TRUE), max(df_step_perf$step, na.rm = TRUE), by = 0.2),1)) + 
        theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
dat4 = ggplot_build(p4)$data[[1]]
p4 = p4 + geom_segment(data=dat4, aes(x=xmin, xend=xmax, y=middle, yend=middle), colour="white", size=0.7)
cache("p4")

pred_grid = grid.arrange(p1, p2, p3, p4, ncol=2)


##Country boxplot
#Derive and sort top 10 countries from df_enrol_ct dataset
ordered <- order(df_enrol_ct$Count, decreasing = TRUE)
top_countries = df_enrol_ct[ordered,][1:20,]
#Filter dataset for top 10 countries
df_step_perf_ct = df_step_perf %>%
        filter(detected_country %in% top_countries$Country) %>%
        mutate(step = ifelse(is.na(step), 0, step)) %>%
        group_by(detected_country) %>%
        summarise(step = mean(step))
        # arrange(step)
df_step_perf_ct$detected_country = countrycode(df_step_perf_ct$detected_country, "iso2c", "country.name")
cache("df_step_perf_ct")
####Predictor Country
ggplot(data = df_step_perf_ct, mapping = aes(x = reorder(detected_country, step), y = step)) + geom_col(fill=I("#0066CC"), col=I("white"), alpha=I(0.8)) +
        theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5)) + 
        labs(x = "Country", y = "Step", title = "Step Performance by Country (Top 20)")


################################ LM Var Tests  ###########################################

#Ages
levs = levels(df_predictors$age_range)
p = c()
for(i in 1:length(levs)){
        x = dim(filter(df_predictors, age_range == levs[i] & completed == 1))[1]
        y = dim(filter(df_predictors, age_range == levs[i]))[1]
        z = x/y
        p[i] = log(z/ (1 - z))
}
plot(p)

#Education
levs = levels(df_predictors$highest_education_level)
p = c()
for(i in 1:length(levs)){
        x = dim(filter(df_predictors, highest_education_level == levs[i] & completed == 1))[1]
        y = dim(filter(df_predictors, highest_education_level == levs[i]))[1]
        z = x/y
        p[i] = log(z/ (1 - z))
}
plot(p)

############################## Linear Modelling  #########################################

### Predictive Modelling

df_predictors = df_step_perf

#Transform predictor dataset
df_predictors = df_predictors %>%
        #Remove extra vars
        dplyr::select(-(learner_id:purchased_statement_at)) %>%
        dplyr::select(-(employment_area:days_to_completion)) %>%
        dplyr::select(-(step:purchased))


#Convert nominal categorical var to binary integer
df_predictors$gender = (as.integer(df_predictors$gender)-1)  
#Convert categorical vars to integers as per results of ordinalt tests (ref. Ordinal_test.R))
df_predictors$age_range = as.integer(df_predictors$age_range)
df_predictors$highest_education_level = as.integer(df_predictors$highest_education_level)



###### PRELIM MODEL
#Deconstruct, standardise and rebuild predictors dataset (& remove employment status because of conversion difficulty)
df_predictors_std = scale(as.matrix(df_predictors[,1:3]))
df_predictors_lm = data.frame(df_predictors_std, df_predictors[,5])
names(df_predictors_lm) = c("gender","age_range","highest_education_level","completed")

#Fit the Linear model using the dataset
lr_fit = glm(completed ~ ., data = df_predictors_lm, family = "binomial")
summary(lr_fit)
#Compute prediction probabilities
phat = predict(lr_fit, df_predictors_lm, type = "response")
#Compute fitted (i.e. predicted) values
yhat = ifelse(phat > 0.5, 1, 0)
#Calculate confusion matrix
table(Observed=df_predictors_lm$completed, Predicted = yhat)
#Compute training error
1 - mean(df_predictors_lm$completed == yhat)




###### FINAL MODEL 

#Final model Age + Gender
df_predictors_lm_red = df_predictors_lm[,-3]
names(df_predictors_lm_red) = c("gender","age_range","completed")

set.seed(20)
#Create training and testing sets
len_set = dim(df_predictors_lm_red)[1]
train_qt = round(len_set*0.9,0)
random_sel = sample(len_set, len_set, replace = FALSE)
#Create test/training indices
train_sel = random_sel[1:train_qt]
test_sel = random_sel[(train_qt+1):len_set]
#Slice predictors df into test/training dfs
df_train = filter(df_predictors_lm_red, row_number() %in% train_sel)
df_test = filter(df_predictors_lm_red, row_number() %in% test_sel)

#Fit the Linear model using the dataset
lr_fit_final = glm(completed ~ ., data = df_train, family = "binomial")

cache("lr_fit_final")

summary(lr_fit_final)
#Compute prediction probabilities
phat_test = predict(lr_fit_final, df_test, type = "response")
#Compute fitted (i.e. predicted) values
yhat_test = ifelse(phat_test > 0.5, 1, 0)
#Calculate LR confusion matrix
conf_matrix = table(Observed=df_test$completed, Predicted = yhat_test)
#Compute LR test error
lr_test_error = 1 - mean(df_test$completed == yhat_test)

cache("conf_matrix")
cache("lr_test_error")



###### CROSS VALIDATION

n=nrow(df_predictors_lm_red)

set.seed(20)
#10-fold cross validation
nfolds = 10
#Sample fold-assingment index
fold_index = sample(nfolds, n, replace=TRUE)
fold_sizes = numeric(nfolds)
#Compute fold sizes
for(k in 1:nfolds){
        fold_sizes[k] = length(which(fold_index==k))
        fold_sizes
}

#Assign vector for avg MSE
cv_lsq_errors = numeric(nfolds)

#Loop through folds fitting model to k-1 training data and predicting values for k, assign errors to vector
for(k in 1:nfolds){
        #Fit model by least squares using all but the k-th fold
        lsq_tmp_fit = glm(completed ~ ., data=df_predictors_lm_red[fold_index!=k,], family = "binomial")
        #Compute fitted values for the k-th fold
        phat = predict(lsq_tmp_fit, df_predictors_lm_red[fold_index == k,], type = "response")
        #Work out the MSE for the k-th fold
        yhat = ifelse(phat > 0.5, 1, 0)
        cv_lsq_errors[k] = mean((df_predictors_lm_red[fold_index==k,]$completed - yhat)^2)
}

#Take mean of error vector
cross_val_test_error = weighted.mean(cv_lsq_errors, w=fold_sizes)

cache("cross_val_test_error")