---
title: "Analysis of Terascope Benchmarking Data"
author: "Mark R. Tyrrell"
date: "23/01/2019"
output:
  pdf_document: null
  html_document:
    df_print: paged
header-includes:
- \usepackage{graphicx}
- \usepackage{float}
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
knitr::opts_chunk$set(fig.pos = 'H')
# knitr::kable()
```



```{r load_project, include=FALSE}
library(ProjectTemplate);load.project()

```




```{r, echo=FALSE}

```

# 1. Introduction

 Visualization methods
panoramic images show potential for visualizing multiscale urban data. accessible route to supercomputer visualizations on many low cost devices.
Visual supercomputing
deployed 14 PFlop cloud supercomputer, larger than any GPU HPC system in UK. cost in total for one result (a scaling graph) ~£20,000 using > £10 million computer. Azure improved (from K80 to V100) during the project, we immediately benefited. provides access to performance approx. 20 years ahead of current desktop systems.

```{r,eda_f01a, echo=FALSE, fig.cap = "Newcastle University Terascope", fig.pos = "H", out.width='80%', fig.asp=0.75, fig.align='center', cache=TRUE}
knitr::include_graphics(path = "../graphs/terascope.jpg")
```

# 2. Background

Terapixel images offer an intuitive, accessible way to present information sets to stakeholders, allowing viewers to interactively browse big data across multiple scales. The challenge we addressed here is how to deliver the supercomputer scale resources needed to compute a realistic terapixel visualization of the city of Newcastle upon Tyne and its environmental data as captured by the Newcastle Urban Observatory.

Our solution is a scalable architecture for cloud-based visualization that we can deploy and pay for only as needed. The three key objectives of this work are to: create a supercomputer architecture for scalable visualization using the public cloud; produce a terapixel 3D city visualization supporting daily updates; undertake a rigorous evaluation of cloud supercomputing for compute intensive visualization applications.

We demonstrate that it is feasible to produce a high quality terapixel visualization using a path tracing renderer in under a day using public IaaS cloud GPU nodes. Once generated the terapixel image supports interactive browsing of the city and its data at a range of sensing scales from the whole city to a single desk in a room, accessible across a wide range of thin client devices.

Problem Description

The dataset below was created from application checkpoint and system metric output from the production of a terapixel image. There are a variety of problems which can be addressed using the TeraScope dataset. Each will commence with an exploratory data analysis. Examples of questions you may wish to be able to answer through the EDA process are as follows:

Which event types dominate task runtimes?
What is the interplay between GPU temperature and performance?
What is the interplay between increased power draw and render time?
Can we quantify the variation in computation requirements for particular tiles?
Can we identify particular GPU cards (based on their serial numbers) whose performance differs to other cards? (i.e. perpetually slow cards).
What can we learn about the efficiency of the task scheduling process?
A project may focus entirely on the EDA process, or you may also wish to develop a data product to allow interactive exploration of the data. For example, you could develop a Shiny dashboard to display the results of your analysis into render performance and system operation, displaying timelines of execution based on the application checkpoint data.





# 3. Methodology
Data analysis was conducted using the R programming language and the R Studio IDE, with the report composed in Rmarkdown. The ProjectTemplate library was used to organise the project and ensure reproducability. Version control was managed using git and Github. 

An archived folder is available containing the original Rmarkdown .rmd file and git log. The folder is structured for standard data mining projects, including subfolders for data, cache, munge and source files. As the data analysis involves operations which require extended processing time, key objects have been cached as .Rdata files. The objects are called along the progression of the script (and Rmarkdown report), while the original code is included in the munge folder. Detailed instructions for reproducing the analysis are included in the main README file.

The original csv data files are too large for Github at 300+ MB. Therefore they are not included in the archived project folder. A download link can be found in the README file. 

## 3.1 Data Understanding

The data made available for this analysis consists of 3 tabular data files in comma-separated value (.csv) format covering multiple aspects of the terapixel rendering process. The data files are detailed below.

### 3.1.1 Application Checkpoints (application-checkpoints.csv 111.2MB)
This file contains 660,400 observations over 6 variables detailing application checkpoint events throughout the execution of the render job. 

1. **timestamp**   Timestamp of observation entry
2. **hostname**    Unique hostname of the virtual machine auto-assigned by the Azure batch system (Total unique: 1024)
3. **eventName**   Name of the event occuring within the rendering application. Possible Values:
    + **Tiling**    Post processing of the rendered tile
    + __Saving__    Measure of configuration overhead
    + __Render__    Actual rendering operation
    + __TotalRender__ Complete task checkpoints
    + __Uploading__ Output from post processing uploaded to Azure Blob Storage

4. **eventType** Possible Values: __START__, __STOP__
5. **jobId** ID of the Azure batch job (Total: 3 one for each level 4, 8 and 12)
6. **taskId** ID of the Azure batch task (Total: 65793 level 4 1x1; level 8 16x16; level 12 256x256)

### 3.1.2 GPU (gpu.csv 208.7MB)
This file contains 1,543,681 observations over 8 variables detailing logged metrics for each GPU. The log entries occur on a schedule of approximately every 2 seconds for each GPU.

1. **timestamp**   Timestamp of observation entry
2. **hostname**    Unique hostname of the virtual machine auto-assigned by the Azure batch system (Total unique: 1024)
3. **gpuSerial** Unique serial number of the physical GPU card (Total unique: 1024)
4. **gpuUUID** Unique system id assigned by the Azure system to the GPU unit (Total unique: 1024)
5. **powerDrawWatt** Power draw of the GPU in watts
6. **gpuTempC** Temperature of the GPU in Celsius
7. **gpuUtilPerc** Percent utilisation of the GPU Core(s)
8. **gpuMemUtilPerc** Percent utilisation of the GPU memory

### 3.1.3 Task X Y (task-x-y.csv 6.2MB)
This file contains 65,793 observations over 5 variables detailing the tile x,y co-ordinates of which part the image was being rendered for each task.

1. **jobId** ID of the Azure batch job (Total: 3 one for each level 4, 8 and 12)
2. **taskId** ID of the Azure batch task (Total: 65793 level 4 1x1; level 8 16x16; level 12 256x256)
3. **x** X co-ordinate of the image tile being rendered (Total unique: 256)
4. **y** Y co-ordinate of the image tile being rendered (Total unique: 256)
5. **level** 3 zoomable "google maps style" map levels. 12 levels in totla. Level 1 is zoomed right out and level 12 is zoomed right in. Only levels 4, 8 and 12 in the dataset as the intermediate levelx are derived during the tiling process.


## 3.2 Data Preparation

The csv files were read into R and reviewed. Data types were changed as required (eg. timestamps, factors to char etc). In the case of the GPU dataset, the extraneous column **gpuUUID** was dropped, as it was redundant with **gpuSerial** as a unique identifier. The Task X Y dataset was fully merged into the Application Checkpoint dataset, as it contained only 3 unique columns. The resulting object **app_task** was cached along with the **gpu** object for the GPU dataset, after checking for NAs. These cleaned and cached .Rdata objects form the basis of the analysis.

The analysis made heavy use of the Dplyr package to transform and filter **app_task** and **gpu**. Multiple analyses were based on transformed datasets that required extensive processing time. These datasets were cached as .Rdata objects, with the original code included in the munge folder. This function was particularly exemplified in the form of the **gpu_task** dataset, which was the output of the computation of mean resource usage for each event under each of the 65793 tasks in the **app_task** dataset. This computation took three hours on a Corei5 system.


# 4. Exploratory Data Analysis

As detailed in section 2.2, the data show 1024 unique VMs, each with an attached GPU. These 1024 GPUs were used to render 65,793 pixel generation operations identified by a unique task ID. The four phases of pixel generation in order of execution are: Saving Config, Rendering, Tiling and Uploading. Each of these phases is logged in the **app_task** dataset, once at the start of the phase, and once at the end. In addition, a fifth entry is made for the total rendering process. This results in 10 observations for each pixel generation. Execution times for each phase of each pixel render were derived from the application checkpoint timestamps (**app_task**)

## 4.1 GPU Single Unit Examination

In order to understand the dataset better, a single GPU was selected at random from the **gpu** dataset (S/N: 323617042631). The logged data for this GPU over the first 10 minutes of processing are displayed in Figure 1. The cycles for each pixel render can be clearly seen in the plot for each metric. Each pixel render appears to take approximately 40 seconds. GPU utlisation percentage appears relatively uniform for each pixel render at approximately 90%, whereas there is more variation on the other metrics. This is possibly a result of a percentage usage software limit on the GPU. Unsurprisingly, the temperature builds up from a cold start over successive pixel renders. 

```{r eda_f01, fig.cap = "Sample GPU Log Metrics over Time", fig.pos = "H", out.width='80%', fig.asp=0.75, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE, results= 'hide', , cache=TRUE}
# Create index column with accessible numbers for each gpu serial number
gpu_id = data.frame(gpuSerial = sort(unique(gpu$gpuSerial)), id =  1:1024)
# Create a copy of gpu dataset and insert gpu index vector
gpu_r = gpu
gpu_r = left_join(gpu_r, gpu_id)

# Filter gpu_r dataset by unique gpu index number for plotting
gpu_t = gpu_r %>%
        filter(id == 500) %>%
        arrange(timestamp) %>%
        filter(row_number() < 300)

# Plot each metric in the subsetted gpu_r dataset
p1 = ggplot(gpu_t, aes(timestamp, powerDrawWatt)) + geom_line(color='#0066CC') + labs( x = 'timestamp (s)', y = 'Power (W)')
p2 = ggplot(gpu_t, aes(timestamp, gpuTempC)) + geom_line(color='#0066CC') + labs(x = 'timestamp (s)', y = 'Temperature (C)')
p3 = ggplot(gpu_t, aes(timestamp, gpuUtilPerc)) + geom_line(color='#0066CC') + labs(x = 'timestamp (s)', y = 'GPU Usage (%)')
p4 = ggplot(gpu_t, aes(timestamp, gpuMemUtilPerc)) + geom_line(color='#0066CC') + labs(x = 'timestamp (s)', y = 'Memory Usage (%)')
# Plot grid of 4
grid.arrange(p1, p2, p3, p4, ncol=2)
```

## 4.2 GPU Performance and Log Metrics Correlation

Figure 2 displays scatter plots of the GPU log metrics correlations. As the GPU dataset is extremely large at 1.5 million observations, a random sample of 10,000 was selected for the plots. As demonstrated, there is a postive correlation between all of the metrics. However, there is also significant variation. This finding matches the observations taken from Figure 1. The positive correlation is to be expected, as the increased workload on the GPU in the form of GPU utilisation percentage will naturally require associated memory for the tiling job, and the demands of this utilisation on the hardware components will result in increased power consumption and rising temperature. 

```{r eda_f02, fig.cap = "GPU Log Metrics Correlation", fig.pos = "H", out.width='80%', fig.asp=0.75, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE, results= 'hide', cache=TRUE}
# Q2 interplay between GPU Performance metrics
q2 = gpu %>%
        select(4:7)

# Take sample of q2 index
x = sample(1:dim(q2)[1],10000)
# Slice q2 by sample
q2_samp = q2[x,]

p5 = ggplot(q2_samp, aes(powerDrawWatt, gpuTempC)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Power (W)', y = 'Temperature (C)')
p6 = ggplot(q2_samp, aes(gpuUtilPerc, gpuTempC)) + geom_point(size=0.25) + stat_smooth()+ labs( x = 'GPU Usage (%)', y = 'Temperature (C)')
p7 = ggplot(q2_samp, aes(gpuUtilPerc, powerDrawWatt)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'GPU Usage (%)', y = 'Power (W)')
p8 = ggplot(q2_samp, aes(gpuUtilPerc, gpuMemUtilPerc)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'GPU Usage (%)', y = 'Memory Usage (%)')
p9 = ggplot(q2_samp, aes(powerDrawWatt, gpuMemUtilPerc)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Power (W)', y = 'Memory Usage (%)')
p10 = ggplot(q2_samp, aes(gpuTempC, gpuMemUtilPerc)) + geom_point(size=0.25) + stat_smooth() + labs( x = 'Temperature (C)', y = 'Memory Usage (%)')

# Plot grid of 6
grid.arrange(p5, p6, p7, p8, p9, p10, ncol=3)
```

## 4.3 GPU Performance and Log Metrics Analysis

### 4.3.1 Summary Statistics

Table 1 displays the mean of each GPU log metric by each phase in the pixel rendering process. The rendering task is easily identifiable as the most energy-intense phase of pixel generation, with power consumption more than double that of the second highest phase. There is much less variation in relative temperatures, though rendering is still noteably higher in mean temperature than the other phases. 

```{r eda_t01, echo=FALSE, cache=TRUE}
# Table showing means of all gpu metrics for each task mean by event
gpu_plot_agg = gpu_task %>%
        filter(eventName != 'TotalRender') %>%
        group_by('Event' = eventName) %>%
        summarise('Power (W)' = round(mean(watt),2), 'Temperature (s)' = round(mean(temp),2), 'GPU Usage (%)' = round(mean(cpu),3), 'Memory Usage (%)' = round(mean(mem),3), n = n())

knitr::kable(gpu_plot_agg, booktabs = TRUE, caption = 'Mean GPU Log Metrics by Phase (All Levels)')

```

There are noteable missing data in Table 1 for the 'Saving Config', 'Uploading' and 'Tiling' phases over the GPU and memory utilisation metrics. The dataset used to compute the table was taken from **gpu_task**. This dataset is a join of **gpu** with **app_task**, where each gpu observation is matched by the VM hostname and timestamp for each phase. Despite the reasonably high resolution of the GPU logging operation (each GPU logged performance metrics approximately every 2 seconds), on average these task phases occur over very short intervals (around 1 second or less). As such, these phases were statistically less likely to occur during coincident GPU logging, and consequently have a much smaller sample size relative to the rendering phase. 

Table 2 provides quantified insight into this effect by detailing the mean runtime durations for each event. The phases in question display very short average durations. This effect is particularily exemplified in the case of Saving Config. With an average interval of 2ms, it is matched in only 108 of the GPU observations.

The presence of power and temperature data for the under-represented phases in the absence of corresponding GPU and memory data remains questionable. The power metric can assumed to be exhibiting baseline consumption. It is likely the temperature metric follows a heat loading dynamic, taking time to cool off after each rendering phase. The negative corrollary of this effect was already observed (ref. Figure 1). With regard to GPU and memory, the phases in question can be assumed to put only relatively low strain on the hardware resources. Therefore it is not surprising to see zero utilisation on these metrics, as any small percent utilisation caught by the sampling process would be effectively zeroed out by the mean operation. 

```{r eda_t02, echo=FALSE, cache=TRUE}
# Q1 Which tasks dominate runtimes?
task_runtime_means = task_runtimes %>%
        # Aggregate by event taking mean of all observations for each event
        filter(eventName != 'TotalRender') %>%
        group_by('Event' = eventName) %>%
        summarise('Duration (s)' = round(mean(duration),3), n = n())

knitr::kable(task_runtime_means, booktabs = TRUE, caption = 'Mean Execution Time by Phase (Level 12 only)')
```

### 4.3.2 Distribution

The summary statisics for the GPU performance and log metrics provide a succinct method for comparing the various phases in terms of impact on system resources. However, they provide only limited insight on actual system performance. A more robust method of benchmarking high performance computational systems is to analyse quantiles including the median[1]. 

Figure 3 displays the distributions of execution time for each pixel generation. The top plot presents the execution time for rendering and the bottom for the tiling phase. Both distributions appear bimodal, with the larger mode appearing mostly normal with a skew to the right. The smaller mode appears to represents a group of GPUs which exhibit significantly higher performance. However, the computational requirements of each pixel rendering are not uniform. Therefore it is impossible to confirm this from these visualisations (cf. Section 4.4)

The features of the render distribution highlight the importance of documenting system performance beyond summary statistics. The long tail to the right is typical of computational systems, where the accumulated effects of networking, scheduling and processing error push the upper bound towards a log-normal distribution[1]. Were a normal distribution assumed in this case, the standard deviation for the render execution time distribution would be calculated as 6.047s. The maximum execution time at 81.51s would then be a highly improbable 6.7 sigma event.

```{r eda_f03, fig.cap = "Distribution by Execution Time: Render \\& Tiling (Level 12)", fig.pos = "H", out.width='80%', fig.asp=0.75, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE, results= 'hide', , cache=TRUE}
##### Execution Time Plot - histograms showing distribution of execution times for each event type (Render & Tiling)
grid.arrange(p12, p13, ncol=1)
```

The bimodal characteristics of the distributions in Figure 3 are more pronounced for the rendering phase distribution; a finding which is logical given the doubled sample size for the rendering phase, as well as the vastly larger quantity of GPU logging data available over the 40+ second phase duration. Additionally as the time scale for the rendering phase is relatively large compared to tiling, it is easier to inherently comprehend the effects of variance on the operation through visualisation. As the comparative performance and relationship between the render phase and the other three phases has already been established in previous sections, the render phase will be used to evaluate system performance exlcusively from this point.

Figures 4 and 5 display the distributions of power consumption, temperature, GPU and memory utilisation for the render phase. The distribution pairs in each figure appear to be similar. This finding matches the observations in Figure 1, where a high degree of correlation was shown between these respective pairs.  

```{r eda_f04, fig.cap = "Distribution by Power Consumption \\& Temperature: Render", fig.pos = "H", out.width='80%', fig.asp=0.75, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE, results= 'hide', cache=TRUE}
grid.arrange(p14, p15, ncol=1)
```

```{r eda_f05, fig.cap = "Distribution by GPU \\& Memory \\% Utilisation: Render", fig.pos = "H", out.width='80%', fig.asp=0.75, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE, results= 'hide', cache=TRUE}
grid.arrange(p16, p17, ncol=1)
```

In contrast to the execution time analysis, the distributions for the GPU logged metrics appear to be more normally distributed. As these data are different are not computational performance measures, it is reasonable to assume they would more closely follow a normal distribution rather than log-normal. The QQ plots in Figure 6 confirm this finding. With the exception of GPU utilisation, the theoretical and sample quantiles for each distribution are closely aligned. Therefore it is safe to model system performance on these metrics based on a normal distribution.




```{r,eda_f07, echo=FALSE, fig.cap = "QQ Plot for GPU Log Metrics", fig.pos = "H", out.width='80%', fig.asp=0.75, fig.align='center', cache=TRUE}
knitr::include_graphics(path = "../graphs/p23.png")
```


## 4.4 Comparison of Pixel Rendering Complexity

Comparing the variation in GPU execution time and log metrics is problematic in that the workload assigned to each GPU is not identical. Each pixel is unique, and therefore the computational cost of rendering each pixel is not uniform. Various complexities in the input including topological structure affect the processing time and resources required to render the pixel. 

A visualisation of the comparative difference in rendering each pixel is presented in Figure 8. The six images are heatmaps displaying the relative impact of rendering each pixel in terms of a particular GPU metric, with lighter colours indicating higher values. The colour mapping is only relative inside each visualisation, i.e. there is no inherent relationship between the colour intensities of the various heatmaps. 

The top row of Figure 8 contains visualisations for the GPU log metrics. When compared with the terascope map (ref. Figure 1), the cartolographic structure of each heatmap is clearly apparent. As can be seen, there are obvious areas common to all three plots in which the associated pixel rendering was less impactful on the metric. For instance, the top left corner of each heatmap contains a rectangular region with darker colour. This region corresponds to the legend (ref. Figure 1) which was actually included in the rendering process. As the legend is a mostly uniform colour graphic, the rendering process for pixels in this area was significantly less intense. 

Additionally the construction site to the south west of St. James Park is clearly articulated as a low-intensity region. This was possibly due to a lack of activity at the time the terascope input data was aquired. The area has since become a major construction site with many new buildings. In general, streets required higher proccessing inputs, making them stand out and articulating the buildings and structures they delineate. This is likely caused by the shadows cast on streets by surrounding buildings. Shadows cause gradient ligting effects, resulting in more information to process than for regions of comparatively uniform light and colour. 

```{r eda_f08, fig.cap = "Distribution by GPU \\& Memory: Render", fig.pos = "H", out.width='100%', fig.asp=0.75, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE, results= 'hide', cache=TRUE}
# knitr::include_graphics(path = "../graphs/p24.pdf")
grid.arrange(i30, i33, i34, i29, i31, i32, ncol = 3)
```

In section 4.3.2 the bimodal distribution of the GPU execution times (render phase) raised questions as to the cause of the secondary mode. This smaller density is centred at approximately 24s, slightly more than half the median of the primary density (41.65s) and contains approximately 3.4 percent of total samples. The heatmap visualisation can be used to identify the cause of this anomoly.

The bottom row of Figure 8 contains various visualisations for execution time. The far left plot contains all 65,536 observations and displays a highly defined representation of the pixel execution time variation. The middle plot is filtered to display only pixels rendered with an execution time greater than the 95th percentile of the distribution (approximately 50s). In alignment with the left hand plot, it appears the major source of execution time delay was light scatter around the St. James Park superstructure. This finding is unsurprising in that the superstructure is multi-faceted and made of highly reflective material.

The far right plot in Figure 8 is filtered to display only pixels with execution times less than 25s. This filter isolates the smaller of the densities in the bimodal distribution of execution times. As demonstrated, the smaller density is a result of the rendering of the terascope legend; already identified as computationally simple task.

 




## 4.5 GPU Trends by Unit

The distributions of the GPU render phase performance and log metrics raise questions as to why 

```{r eda_f09, fig.cap = "Execution Time by GPU S/N", fig.pos = "H", out.width='80%', fig.asp=0.75, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE, results= 'hide', cache=TRUE}
# Plot render time by S/N
h = median(comp_gpu$avg_dur)
ggplot(comp_gpu) + geom_point(aes(gpuSerial, avg_dur)) + labs(x = 'GPU S/N', y = 'Mean Render Time (s)') + 
        geom_hline(yintercept = h, color = 'red', linetype = 'dotdash') +
        annotate("text", x=comp_gpu$gpuSerial[1]+500000000, y=h, label= paste("Median",round(h,2)), size=2.5, vjust=1.5)
```





















## 4. Summary of Results Evaluation and Business Conclusions

The analysis returned numerous results relevant to the business objectives. These results are summarised and evaluated below. Summary conclusions and reccomendations are included where applicable. 

### 4.1 Findings

**GPU loggin should be higher granularity**\newline
There are noteable missing data in Table 1 for the 'Saving Config', 'Uploading' and 'Tiling' phases over the GPU and memory utilisation metrics. The dataset used to compute the table was taken from **gpu_task**. This dataset is a join of **gpu** with **app_task**, where each gpu observation is matched by the VM hostname and timestamp for each phase. Despite the reasonably high resolution of the GPU logging operation (each GPU logged performance metrics approximately every 2 seconds), on average these task phases occur over very short intervals (around 1 second or less). As such, these phases were statistically less likely to occur during coincident GPU logging, and consequently have a much smaller sample size relative to the rendering phase. 

Table 2 provides quantified insight into this effect by detailing the mean runtime durations for each event. The phases in question display very short average durations. This effect is particularily exemplified in the case of Saving Config. With an average interval of 2ms, it is matched in only 108 of the GPU observations.

**GPU logging should allow smaller measures of GPU/Mem usage**\newline
The presence of power and temperature data for the under-represented phases in the absence of corresponding GPU and memory data remains questionable. The power metric can assumed to be exhibiting baseline consumption. It is likely the temperature metric follows a heat loading dynamic, taking time to cool off after each rendering phase. The negative corrollary of this effect was already observed (ref. Figure 1). With regard to GPU and memory, the phases in question can be assumed to put only relatively low strain on the hardware resources. Therefore it is not surprising to see zero utilisation on these metrics, as any small percent utilisation caught by the sampling process would be effectively zeroed out by the mean operation. 



**Distributions of different metrics can allow differing tools**\newline 
In contrast to the execution time analysis, the distributions for the GPU logged metrics appear to be more normally distributed. As these data are different are not computational performance measures, it is reasonable to assume they would more closely follow a normal distribution rather than log-normal. The QQ plots in Figure 6 confirm this finding. With the exception of GPU utilisation, the theoretical and sample quantiles for each distribution are closely aligned. Therefore it is safe to model system performance on these metrics based on a normal distribution.

**Benchmarking performance requires uniform input data - same pixels can be used across all GPUs**\newline 
Comparing the variation in GPU execution time and log metrics is problematic in that the workload assigned to each GPU is not identical. Each pixel is unique, and therefore the computational cost of rendering each pixel is not uniform. Various complexities in the input including topological structure affect the processing time and resources required to render the pixel. 


## Conclusions for future data mining

The large quantity of missing demographic data (approximately 90%) limited meaningful analysis of subsets of the sample. For instance, though there were 289 observations in the dataset of paid learners, only 13 of these obervations had complete demographic variables. Therefore it was impossible to draw statistical inference about demographic factors contributing to paid upgrades. If there was someway of enticing users to enter demographic data more frequently, it could vastly increase the utitlity of the learning analytics for this course. 



## References

1.  http://www.policyconnect.org. uk/hec/sites/site_hec/files/ report/419/fieldreportdownload/ frombrickstoclicks- hecreportforweb.pdf

2. Chuang, Isaac and Ho, Andrew, HarvardX and MITx: Four Years of Open Online Courses -- Fall 2012-Summer 2016 (December 23, 2016). Available at SSRN: https://ssrn.com/abstract=2889436 or http://dx.doi.org/10.2139/ssrn.2889436

3. National Science Foundation, National Center for Science and Engineering Statistics, Scientists and Engineers Statistical Data System (SESTAT) and National Survey of College Graduates (NSCG) (1993, 2013), http://sestat.nsf.gov.

## Appendix: R-Code

#### The code below is provided for reference and reproducibility purposes. The sections detail the individual data files used for the analysis, and the order reflects the loading seqence. 

```{r, echo=TRUE, eval=FALSE}
################################### Munge: 01-A.R ########################################

##### Read csv data files, combine by type (rbind), clean/transform as required and cache as .RData

##### Enrolments
# Load and combine all datasets
list_sel = list.files(path = "data/", pattern = "*enrolments*")
x <- lapply(list_sel, function(i) read.csv(file = paste("data/",i,sep = "")))
df_enrolments <- do.call('rbind', x) 

# Clean & Transform
df_enrolments = df_enrolments %>%
        #Convert date columns to POSIX UTC
        mutate(enrolled_at = as.character(enrolled_at)) %>%
        mutate(unenrolled_at = as.character(unenrolled_at)) %>%
        mutate(fully_participated_at = as.character(fully_participated_at)) %>%
        mutate(purchased_statement_at = as.character(purchased_statement_at)) %>%
        mutate(enrolled_at = ymd_hms(enrolled_at, tz="UTC")) %>%
        mutate(unenrolled_at = ymd_hms(unenrolled_at, tz="UTC")) %>%
        mutate(fully_participated_at = ymd_hms(fully_participated_at, tz="UTC")) %>%
        mutate(purchased_statement_at = ymd_hms(purchased_statement_at, tz="UTC")) %>%
        #Convert country to character vector
        mutate(detected_country = as.character(detected_country)) %>%
        # Add derived variables to display Days Enrolled, Days to Completion, and binary completion variable
        mutate(days_enrolled = as.numeric(round(difftime(unenrolled_at, enrolled_at, unit="days"),0))) %>%
        mutate(days_to_completion = as.numeric(round(difftime(fully_participated_at, enrolled_at, unit="days"),0))) %>%
        mutate(completed = ifelse(is.na(fully_participated_at), 0, 1)) %>%
        mutate(learner_id = as.character(learner_id))

cache("df_enrolments")

##### Step Activity
# Load and combine all datasets
list_sel = list.files(path = "data/", pattern = "*activity*")
x <- lapply(list_sel, function(i) read.csv(file = paste("data/",i,sep = "")))
df_step_activity <- do.call('rbind', x)

# Clean & Transform
df_step_activity = df_step_activity %>%
        #Convert date columns to POSIX UTC
        mutate(first_visited_at = ymd_hms(first_visited_at, tz="UTC")) %>%
        mutate(last_completed_at = ymd_hms(last_completed_at, tz="UTC")) %>%
        mutate(learner_id = as.character(learner_id))

cache("df_step_activity")


##### Functions
#Tabulate function (shows percentage)
tblFun <- function(x){
        tbl <- table(x)
        res <- cbind(tbl,round(prop.table(tbl)*100,2))
        colnames(res) <- c('Count','Percentage')
        res
}

cache("tblFun")

############################### EDA EDA_Enrolment.R ######################################

# DF Step Activity

# Create working object from cached dateset
df_enrol = df_enrolments

cache("df_enrol")


### 3.1 Profile existing student base

#Student data summary statistics (filtering out unknowns and insufficient samples)
df_enrol_dt = filter(df_enrol, gender != "Unknown" & gender != "nonbinary" & gender != "other" 
                     & age_range != "Unknown" & detected_country != "--" & highest_education_level != "Unknown" & employment_status != "Unknown")

#Reorder factor levels and remove unknown
df_enrol_dt$age_range = factor(df_enrol_dt$age_range, c("<18","18-25","26-35","36-45","46-55","56-65",">65"))
df_enrol_dt$gender = factor(df_enrol_dt$gender, c("male", "female"))
df_enrol_dt$highest_education_level = factor(df_enrol_dt$highest_education_level, c("apprenticeship","less_than_secondary","professional","secondary","tertiary","university_degree","university_masters","university_doctorate"))
df_enrol_dt$employment_status = factor(df_enrol_dt$employment_status, c("full_time_student","looking_for_work","not_working","retired","self_employed","unemployed","working_full_time","working_part_time"))
#Rename factor levels
df_enrol_dt$highest_education_level = mapvalues(df_enrol_dt$highest_education_level, 
        from = c("apprenticeship","less_than_secondary","professional","secondary","tertiary","university_degree","university_masters","university_doctorate"), 
        to = c("apprentice","< secondary","professional","secondary","tertiary","bachelors","masters","doctorate"))
df_enrol_dt$employment_status = mapvalues(df_enrol_dt$employment_status,
        from = c("full_time_student","looking_for_work","not_working","retired","self_employed","unemployed","working_full_time","working_part_time"),
        to = c("student","seeking work","not working","retired","self-employed","unemployed","full-time work","part-time work"))


cache("df_enrol_dt")


#Gender histogram
ggplot(df_enrol_dt) + stat_count(aes(gender), fill=I("#0066CC"), col=I("white"), alpha=I(0.8)) +
        labs(x = "Gender", y = "Count", title = "Gender")

tblFun(df_enrol_dt$gender)

#Age group histogram
ggplot(df_enrol_dt) + stat_count(aes(age_range), fill=I("#0066CC"), col=I("white"), alpha=I(0.8)) +
        labs(x = "Age Groups", y = "Count", title = "Age Groups")

tblFun(df_enrol_dt$age_range)

#Education histogram
ggplot(df_enrol_dt) + stat_count(aes(highest_education_level), fill=I("#0066CC"), col=I("white"), alpha=I(0.8)) +
        labs(x = "Education", y = "Count", title = "Education")

tblFun(df_enrol_dt$highest_education_level)

#Employment histogram
ggplot(df_enrol_dt) + stat_count(aes(employment_status), fill=I("#0066CC"), col=I("white"), alpha=I(0.8)) +
        labs(x = "Employment Status", y = "Count", title = "Employment Status")

tblFun(df_enrol_dt$employment_status)

#Country
#Transform data for detected country, aggregate by count for each country, select top countries by x amount
df_enrol_ct = df_enrol %>%
        dplyr::select(detected_country) %>%
        filter(detected_country != "--") %>%
        mutate(count = 1) %>%
        group_by(detected_country) %>%
        summarise(sum(count)) %>%
        rename(Country = detected_country, Count = "sum(count)") %>%
        mutate(Percentage = round(Count/sum(Count)*100,2)) %>%
        filter(Count > 95) %>%
        arrange(desc(Count))

cache("df_enrol_ct")

#Convert to names for table
df_enrol_ct_name = df_enrol_ct
df_enrol_ct_name$Country = countrycode(df_enrol_ct_name$Country, "iso2c", "country.name")
cache("df_enrol_ct_name")

#Plot and table
ggplot(df_enrol_ct_name, aes(Country, Count)) + geom_col(fill=I("#0066CC"), col=I("white"), alpha=I(0.8)) + 
        labs(x = "Country", y = "Count", title = "Nationality")

df_enrol_ct_name[1:10,]

#Create map object        
map_country = invisible(joinCountryData2Map(df_enrol_ct, joinCode = "ISO2", nameJoinColumn = "Country", verbose = FALSE))
cache("map_country")
#creating a user defined colour palette
# op = palette(c('green','yellow','orange','red'))
# #find quartile breaks
# cutVector = quantile(df_enrol_ct$Count, prob = seq(0, 1, length = 11), type = 5)
# #classify the data to a factor
# df_enrol_ct$Count = cut(df_enrol_ct$Count, cutVector, include.lowest = TRUE)
# #rename the categories
# levels(df_enrol_ct$Count) = c('low', 'med', 'high', 'vhigh', 't1', 't2','t3','t4','t5','t6')
# #mapping by shade
# par(mai=c(0,0,0.2,0),xaxs="i",yaxs="i")
mapCountryData(map_country, nameColumnToPlot = "Count", catMethod='logFixedWidth', mapTitle='Learners by Top 50 Countries', colourPalette = "heat", oceanCol='lightblue', missingCountryCol='white', aspect = 1.5)

#mapping by bubble
# par(mai=c(0,0,0.2,0),xaxs="i",yaxs="i")
# mapBubbles(dF=map_country, nameZSize="Count", nameZColour="Count", numCats = 10, catMethod='categorical', colourPalette='white2Black', oceanCol='lightblue', landCol='wheat', addColourLegend = FALSE, mapTitle='Test')

### 3.2 Performance Metrics

##### Completion Data (n = 2154)
df_enrol_days_cp = filter(df_enrol, !is.na(days_to_completion))

cache("df_enrol_days_cp")

#Plot Course Completion Duration frequency
ggplot(df_enrol_days_cp, aes(days_to_completion)) + geom_histogram(binwidth = 10, fill=I("#0066CC"), col=I("white"), alpha=I(0.8)) + 
        labs(x = "Days", y = "Count", title = "Histogram: Successful Course Completion Duration (days)") + theme(text = element_text(size=9)) +
        scale_x_continuous(breaks = round(seq(min(df_enrol_days_cp$days_to_completion), max(df_enrol_days_cp$days_to_completion), by = 25),1))
ggsave(file.path('graphs', 'course_completion_duration.pdf'))


############################### EDA EDA_Step_Activity.R ##################################

#DF Step Activity

# Create working object from cached dateset
df_step = df_step_activity

# 1. Analysis of Completion Duration by Step
#Compute number of average days take to complete each step
df_step_avg = df_step %>%
        #Add derived variable for duration per step
        mutate(total_days = as.numeric(round(difftime(last_completed_at, first_visited_at, unit="days"),0))) %>%
        #Strip all variables except step and total_days
        dplyr::select(step, total_days) %>%
        #Aggregate
        group_by(step) %>%
        summarise(total_days = round(mean(total_days, na.rm = TRUE),1)) %>%
        #Remove sub-levels
        filter(step < 1.11 | step > 1.19) %>%
        filter(step < 2.11 | step > 2.19) %>%
        filter(step < 2.21 | step > 2.29) %>%
        filter(step < 3.11 | step > 3.19)

cache("df_step_avg")

#Plot average days per step
ggplot(df_step_avg, aes(step, total_days)) + geom_col(size = 1, fill=I("#0066CC"), colour = "#0066CC", alpha=I(0.8)) + labs(x = "Step", y = "Days", 
        title = "Average Completion Duration per Step (days)")  + 
        scale_x_continuous(breaks = round(seq(min(df_step_avg$step), max(df_step_avg$step), by = 0.1),1))
ggsave(file.path('graphs', 'avg_comp_duration_step.pdf'))


# 2. Analysis of Dropout Frequency by Step
df_step_max = df_step %>%
        #Compute maximum level completed for each unique learner ID
        filter(is.na(last_completed_at)) %>%
        dplyr::select(learner_id, step) %>%
        group_by(learner_id) %>%
        summarise(step = max(step))

cache("df_step_max")

df_step_drop = df_step_max %>%
        # Add Count variable (= 1 for each row) and drop learner_id
        mutate(Count = ifelse(!is.na(learner_id), 1, 0)) %>%
        mutate(learner_id = NULL) %>%
        # Remove sub-levels
        filter(step < 1.11 | step > 1.19) %>%
        filter(step < 2.11 | step > 2.19) %>%
        filter(step < 2.21 | step > 2.29) %>%
        filter(step < 3.11 | step > 3.19)
        
cache("df_step_drop")
 
#Plot frequency of drop out by step number
ggplot(df_step_drop, aes(step)) + geom_histogram(binwidth = 0.1, fill=I("#0066CC"), col=I("white"), alpha=I(0.8)) + 
        labs(x = "Step", y = "Count", title = "Histogram: Last Step Completed") +
        scale_x_continuous(breaks = round(seq(min(df_step_drop$step), max(df_step_drop$step), by = 0.1),1)) +
        theme(text = element_text(size=10)) 
ggsave(file.path('graphs', 'dropout_by_step.pdf'))

#Percentage dropout by step 1.1
dim(filter(df_step_drop, step == 1.1))[1]/dim(df_step_drop)[1]


################################# EDA EDA_Join.R #########################################


### Join enrolments data (age, gender etc) with step activity on learner_id and look at performance based on demographics etc (n= 3995)
# Left join on enrolements data (as opposed to step activity data) because demographic (interesting) data is complete. 
#Dataset containing full demographic variables (n = 3819)
df_step_perf = left_join(df_enrol_dt, df_step_max, copy = FALSE)
df_step_perf = df_step_perf %>%
        #Drop unused columns
        dplyr::select(-(enrolled_at:fully_participated_at)) %>%
        mutate(country = NULL) %>%
        #Convert all "completed" steps to 3.9 (otherwise shows NA in some cases)
        mutate(step = ifelse(is.na(step) & completed == 1, 3.9, step)) %>%
        #Create new binary var to show course purchase
        mutate(purchased = ifelse(!is.na(purchased_statement_at),1,0))

cache("df_step_perf")

####Predictor Gender
p1 = ggplot(data = df_step_perf, mapping = aes(x = gender, y = step)) + geom_boxplot(fill=I("#0066CC"), col=I("darkgrey"), alpha=I(0.8), size=0.7) + 
        labs(x = "Gender", y = "Step", title = "a) Gender") + 
        scale_y_continuous(breaks = round(seq(min(df_step_perf$step, na.rm = TRUE), max(df_step_perf$step, na.rm = TRUE), by = 0.2),1))
dat1 = ggplot_build(p1)$data[[1]]
p1 = p1 + geom_segment(data=dat1, aes(x=xmin, xend=xmax, y=middle, yend=middle), colour="white", size=0.7)
cache("p1")

####Predictor Age
p2 = ggplot(data = df_step_perf, mapping = aes(x = age_range, y = step)) + geom_boxplot(fill=I("#0066CC"), col=I("darkgrey"), alpha=I(0.8), size=0.7) + 
        labs(x = "Age Range", y = "Step", title = "b) Age Range") +
        scale_y_continuous(breaks = round(seq(min(df_step_perf$step, na.rm = TRUE), max(df_step_perf$step, na.rm = TRUE), by = 0.2),1))
dat2 = ggplot_build(p2)$data[[1]]
p2 = p2 + geom_segment(data=dat2, aes(x=xmin, xend=xmax, y=middle, yend=middle), colour="white", size=0.7)
cache("p2")

####Predictor Education
p3 = ggplot(data = df_step_perf, mapping = aes(x = highest_education_level, y = step)) + geom_boxplot(fill=I("#0066CC"), col=I("darkgrey"), alpha=I(0.8), size=0.7) + 
        labs(x = "Education Level", y = "Step", title = "c) Education") +
        scale_y_continuous(breaks = round(seq(min(df_step_perf$step, na.rm = TRUE), max(df_step_perf$step, na.rm = TRUE), by = 0.2),1)) +
        theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
dat3 = ggplot_build(p3)$data[[1]]
p3 = p3 + geom_segment(data=dat3, aes(x=xmin, xend=xmax, y=middle, yend=middle), colour="white", size=0.7)
cache("p3")

####Predictor Employment Status
p4 = ggplot(data = df_step_perf, mapping = aes(x = employment_status, y = step)) + geom_boxplot(fill=I("#0066CC"), col=I("darkgrey"), alpha=I(0.8), size=0.7) + 
        labs(x = "Employment Status", y = "Step", title = "d) Employment Status") +
        scale_y_continuous(breaks = round(seq(min(df_step_perf$step, na.rm = TRUE), max(df_step_perf$step, na.rm = TRUE), by = 0.2),1)) + 
        theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
dat4 = ggplot_build(p4)$data[[1]]
p4 = p4 + geom_segment(data=dat4, aes(x=xmin, xend=xmax, y=middle, yend=middle), colour="white", size=0.7)
cache("p4")

pred_grid = grid.arrange(p1, p2, p3, p4, ncol=2)


##Country boxplot
#Derive and sort top 10 countries from df_enrol_ct dataset
ordered <- order(df_enrol_ct$Count, decreasing = TRUE)
top_countries = df_enrol_ct[ordered,][1:20,]
#Filter dataset for top 10 countries
df_step_perf_ct = df_step_perf %>%
        filter(detected_country %in% top_countries$Country) %>%
        mutate(step = ifelse(is.na(step), 0, step)) %>%
        group_by(detected_country) %>%
        summarise(step = mean(step))
        # arrange(step)
df_step_perf_ct$detected_country = countrycode(df_step_perf_ct$detected_country, "iso2c", "country.name")
cache("df_step_perf_ct")
####Predictor Country
ggplot(data = df_step_perf_ct, mapping = aes(x = reorder(detected_country, step), y = step)) + geom_col(fill=I("#0066CC"), col=I("white"), alpha=I(0.8)) +
        theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5)) + 
        labs(x = "Country", y = "Step", title = "Step Performance by Country (Top 20)")


################################ LM Var Tests  ###########################################

#Ages
levs = levels(df_predictors$age_range)
p = c()
for(i in 1:length(levs)){
        x = dim(filter(df_predictors, age_range == levs[i] & completed == 1))[1]
        y = dim(filter(df_predictors, age_range == levs[i]))[1]
        z = x/y
        p[i] = log(z/ (1 - z))
}
plot(p)

#Education
levs = levels(df_predictors$highest_education_level)
p = c()
for(i in 1:length(levs)){
        x = dim(filter(df_predictors, highest_education_level == levs[i] & completed == 1))[1]
        y = dim(filter(df_predictors, highest_education_level == levs[i]))[1]
        z = x/y
        p[i] = log(z/ (1 - z))
}
plot(p)

############################## Linear Modelling  #########################################

### Predictive Modelling

df_predictors = df_step_perf

#Transform predictor dataset
df_predictors = df_predictors %>%
        #Remove extra vars
        dplyr::select(-(learner_id:purchased_statement_at)) %>%
        dplyr::select(-(employment_area:days_to_completion)) %>%
        dplyr::select(-(step:purchased))


#Convert nominal categorical var to binary integer
df_predictors$gender = (as.integer(df_predictors$gender)-1)  
#Convert categorical vars to integers as per results of ordinalt tests (ref. Ordinal_test.R))
df_predictors$age_range = as.integer(df_predictors$age_range)
df_predictors$highest_education_level = as.integer(df_predictors$highest_education_level)



###### PRELIM MODEL
#Deconstruct, standardise and rebuild predictors dataset (& remove employment status because of conversion difficulty)
df_predictors_std = scale(as.matrix(df_predictors[,1:3]))
df_predictors_lm = data.frame(df_predictors_std, df_predictors[,5])
names(df_predictors_lm) = c("gender","age_range","highest_education_level","completed")

#Fit the Linear model using the dataset
lr_fit = glm(completed ~ ., data = df_predictors_lm, family = "binomial")
summary(lr_fit)
#Compute prediction probabilities
phat = predict(lr_fit, df_predictors_lm, type = "response")
#Compute fitted (i.e. predicted) values
yhat = ifelse(phat > 0.5, 1, 0)
#Calculate confusion matrix
table(Observed=df_predictors_lm$completed, Predicted = yhat)
#Compute training error
1 - mean(df_predictors_lm$completed == yhat)




###### FINAL MODEL 

#Final model Age + Gender
df_predictors_lm_red = df_predictors_lm[,-3]
names(df_predictors_lm_red) = c("gender","age_range","completed")

set.seed(20)
#Create training and testing sets
len_set = dim(df_predictors_lm_red)[1]
train_qt = round(len_set*0.9,0)
random_sel = sample(len_set, len_set, replace = FALSE)
#Create test/training indices
train_sel = random_sel[1:train_qt]
test_sel = random_sel[(train_qt+1):len_set]
#Slice predictors df into test/training dfs
df_train = filter(df_predictors_lm_red, row_number() %in% train_sel)
df_test = filter(df_predictors_lm_red, row_number() %in% test_sel)

#Fit the Linear model using the dataset
lr_fit_final = glm(completed ~ ., data = df_train, family = "binomial")

cache("lr_fit_final")

summary(lr_fit_final)
#Compute prediction probabilities
phat_test = predict(lr_fit_final, df_test, type = "response")
#Compute fitted (i.e. predicted) values
yhat_test = ifelse(phat_test > 0.5, 1, 0)
#Calculate LR confusion matrix
conf_matrix = table(Observed=df_test$completed, Predicted = yhat_test)
#Compute LR test error
lr_test_error = 1 - mean(df_test$completed == yhat_test)

cache("conf_matrix")
cache("lr_test_error")



###### CROSS VALIDATION

n=nrow(df_predictors_lm_red)

set.seed(20)
#10-fold cross validation
nfolds = 10
#Sample fold-assingment index
fold_index = sample(nfolds, n, replace=TRUE)
fold_sizes = numeric(nfolds)
#Compute fold sizes
for(k in 1:nfolds){
        fold_sizes[k] = length(which(fold_index==k))
        fold_sizes
}

#Assign vector for avg MSE
cv_lsq_errors = numeric(nfolds)

#Loop through folds fitting model to k-1 training data and predicting values for k, assign errors to vector
for(k in 1:nfolds){
        #Fit model by least squares using all but the k-th fold
        lsq_tmp_fit = glm(completed ~ ., data=df_predictors_lm_red[fold_index!=k,], family = "binomial")
        #Compute fitted values for the k-th fold
        phat = predict(lsq_tmp_fit, df_predictors_lm_red[fold_index == k,], type = "response")
        #Work out the MSE for the k-th fold
        yhat = ifelse(phat > 0.5, 1, 0)
        cv_lsq_errors[k] = mean((df_predictors_lm_red[fold_index==k,]$completed - yhat)^2)
}

#Take mean of error vector
cross_val_test_error = weighted.mean(cv_lsq_errors, w=fold_sizes)

cache("cross_val_test_error")